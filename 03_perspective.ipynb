{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4496d7",
   "metadata": {},
   "source": [
    "\n",
    "# 03 - Perspective Scoring\n",
    "\n",
    "Attach Perspective API scores to imitation outputs and normalize them for downstream analysis notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e131fcf6",
   "metadata": {},
   "source": [
    "\n",
    "**Goals**\n",
    "- Load the imitation bundle and reuse any existing Perspective runs.\n",
    "- Provide a resumable scorer against Perspective API with defensive retries plus dry-run mode.\n",
    "- Flatten scores into analysis-friendly columns for later topic/safety notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b71f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from typing import Callable, Dict, List, Optional, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "from utils.data_io import load_df_list_pickle, flatten_conversation_bundles, describe_bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29b94cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and run toggles\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "ASSETS_PROCESSED = PROJECT_ROOT / \"assets\" / \"processed\"\n",
    "\n",
    "IMITATION_PATH = ASSETS_PROCESSED / \"combat_threads_with_imitation.pkl\"\n",
    "RESUME_PERSPECTIVE = ASSETS_PROCESSED / \"combat_threads_with_perspective.pkl\"\n",
    "OUTPUT_PATH = ASSETS_PROCESSED / \"combat_threads_with_perspective.pkl\"\n",
    "FLAT_SCORES_PATH = ASSETS_PROCESSED / \"combat_threads_with_perspective_scores.parquet\"\n",
    "\n",
    "PERSPECTIVE_API_KEY = os.getenv(\"PERSPECTIVE_API_KEY\")\n",
    "DRY_RUN = True  # set to False to hit Perspective; requires PERSPECTIVE_API_KEY\n",
    "BATCH_LIMIT = 2  # smoke-test size; set to None for full run\n",
    "RATE_LIMIT_SECONDS = 1.0\n",
    "\n",
    "PERSPECTIVE_DISCOVERY_URL = \"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\"\n",
    "PERSPECTIVE_ATTRIBUTES = [\n",
    "    \"AFFINITY_EXPERIMENTAL\",\n",
    "    \"COMPASSION_EXPERIMENTAL\",\n",
    "    \"CURIOSITY_EXPERIMENTAL\",\n",
    "    \"IDENTITY_ATTACK\",\n",
    "    \"IDENTITY_ATTACK_EXPERIMENTAL\",\n",
    "    \"INSULT\",\n",
    "    \"INSULT_EXPERIMENTAL\",\n",
    "    \"NUANCE_EXPERIMENTAL\",\n",
    "    \"PERSONAL_STORY_EXPERIMENTAL\",\n",
    "    \"PROFANITY\",\n",
    "    \"PROFANITY_EXPERIMENTAL\",\n",
    "    \"REASONING_EXPERIMENTAL\",\n",
    "    \"RESPECT_EXPERIMENTAL\",\n",
    "    \"SEVERE_TOXICITY\",\n",
    "    \"SEVERE_TOXICITY_EXPERIMENTAL\",\n",
    "    \"SEXUALLY_EXPLICIT\",\n",
    "    \"THREAT\",\n",
    "    \"THREAT_EXPERIMENTAL\",\n",
    "    \"TOXICITY\",\n",
    "    \"TOXICITY_EXPERIMENTAL\",\n",
    "]\n",
    "\n",
    "ASSETS_PROCESSED, OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ca0a50",
   "metadata": {},
   "source": [
    "\n",
    "### Asset manifest\n",
    "List which assets are needed, the optional resume file, and where derived artifacts land.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7ddd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = [\n",
    "    {\n",
    "        \"role\": \"input\",\n",
    "        \"path\": IMITATION_PATH,\n",
    "        \"note\": \"Imitation output with imm_1 + imm_1_check (processed bundle).\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"resume_optional\",\n",
    "        \"path\": RESUME_PERSPECTIVE,\n",
    "        \"note\": \"Existing Perspective run to reuse or compare.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"output\",\n",
    "        \"path\": OUTPUT_PATH,\n",
    "        \"note\": \"Bundle with Perspective attributeScores attached.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"artifact_optional\",\n",
    "        \"path\": FLAT_SCORES_PATH,\n",
    "        \"note\": \"Optional flattened scores for quick analysis joins.\",\n",
    "    },\n",
    "]\n",
    "manifest_df = pd.DataFrame(manifest)\n",
    "manifest_df[\"exists\"] = manifest_df[\"path\"].apply(lambda p: Path(p).exists())\n",
    "manifest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d4500",
   "metadata": {},
   "source": [
    "\n",
    "### Inspect source bundle\n",
    "Confirm column layout and reuse any previous Perspective run when present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb41d53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_bundle = load_df_list_pickle(IMITATION_PATH)\n",
    "existing_perspective = load_df_list_pickle(RESUME_PERSPECTIVE) if RESUME_PERSPECTIVE.exists() else None\n",
    "\n",
    "print(\"imitation bundle:\", describe_bundle(imitation_bundle))\n",
    "if existing_perspective is not None:\n",
    "    print(\"existing perspective:\", describe_bundle(existing_perspective))\n",
    "\n",
    "preview = flatten_conversation_bundles(imitation_bundle[:1])\n",
    "display(preview.head())\n",
    "if existing_perspective is not None:\n",
    "    existing_preview = flatten_conversation_bundles(existing_perspective[:1])\n",
    "    display(existing_preview.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3a9e1",
   "metadata": {},
   "source": [
    "\n",
    "### Perspective client and scoring helpers\n",
    "Keeps retries/backoff in one place and defaults to a dry-run stub unless you provide `PERSPECTIVE_API_KEY` and flip `DRY_RUN` to `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca5274",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_perspective_client(api_key: Optional[str]):\n",
    "    if not api_key:\n",
    "        raise RuntimeError(\"Set PERSPECTIVE_API_KEY before running live requests.\")\n",
    "    return discovery.build(\n",
    "        \"commentanalyzer\",\n",
    "        \"v1alpha1\",\n",
    "        developerKey=api_key,\n",
    "        discoveryServiceUrl=PERSPECTIVE_DISCOVERY_URL,\n",
    "        static_discovery=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def dummy_score(text: str, attributes: Sequence[str] = PERSPECTIVE_ATTRIBUTES) -> Dict[str, dict]:\n",
    "    span = len(text)\n",
    "    return {\n",
    "        attr: {\n",
    "            \"summaryScore\": {\"value\": 0.0, \"type\": \"PROBABILITY\"},\n",
    "            \"spanScores\": [\n",
    "                {\"begin\": 0, \"end\": span, \"score\": {\"value\": 0.0, \"type\": \"PROBABILITY\"}}\n",
    "            ],\n",
    "        }\n",
    "        for attr in attributes\n",
    "    }\n",
    "\n",
    "\n",
    "def score_with_backoff(\n",
    "    text: str,\n",
    "    client,\n",
    "    attributes: Sequence[str] = PERSPECTIVE_ATTRIBUTES,\n",
    "    max_retries: int = 5,\n",
    "    pause: float = RATE_LIMIT_SECONDS,\n",
    "    dry_run: bool = DRY_RUN,\n",
    ") -> Optional[Dict[str, dict]]:\n",
    "    if not text:\n",
    "        return None\n",
    "    if dry_run:\n",
    "        return dummy_score(text, attributes=attributes)\n",
    "\n",
    "    request_body = {\n",
    "        \"comment\": {\"text\": text},\n",
    "        \"requestedAttributes\": {attr: {} for attr in attributes},\n",
    "    }\n",
    "    last_error: Optional[HttpError] = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.comments().analyze(body=request_body).execute()\n",
    "            return response.get(\"attributeScores\")\n",
    "        except HttpError as exc:  # noqa: PERF203 (want explicit HttpError handling)\n",
    "            last_error = exc\n",
    "            if exc.resp.status == 429 and attempt < max_retries - 1:\n",
    "                time.sleep(pause * (2 ** attempt))\n",
    "                continue\n",
    "            raise\n",
    "    print(f\"Failed to score after retries: {last_error}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def make_score_fn(dry_run: bool = DRY_RUN, api_key: Optional[str] = PERSPECTIVE_API_KEY):\n",
    "    client = None if dry_run else build_perspective_client(api_key)\n",
    "\n",
    "    def _score(text: str) -> Optional[Dict[str, dict]]:\n",
    "        return score_with_backoff(text, client=client, dry_run=dry_run)\n",
    "\n",
    "    return _score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923d6ab3",
   "metadata": {},
   "source": [
    "\n",
    "### Attach scores to the bundle\n",
    "Reuse prior Perspective results when available and keep the list-of-DataFrames shape intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519c88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_existing_perspective(frame: pd.DataFrame, existing: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    merged = frame.copy()\n",
    "    if existing is not None and \"perspective\" in existing:\n",
    "        merged = merged.join(existing[[\"perspective\"]], how=\"left\", rsuffix=\"_existing\")\n",
    "        if \"perspective_existing\" in merged.columns:\n",
    "            merged[\"perspective\"] = merged[\"perspective_existing\"].combine_first(merged[\"perspective\"])\n",
    "            merged = merged.drop(columns=[\"perspective_existing\"])\n",
    "    return merged\n",
    "\n",
    "\n",
    "def attach_perspective(\n",
    "    frame: pd.DataFrame,\n",
    "    score_fn: Callable[[str], Optional[dict]],\n",
    "    existing: Optional[pd.DataFrame] = None,\n",
    ") -> pd.DataFrame:\n",
    "    merged = merge_existing_perspective(frame, existing)\n",
    "    merged[\"perspective\"] = merged.apply(\n",
    "        lambda row: row[\"perspective\"]\n",
    "        if isinstance(row.get(\"perspective\"), dict)\n",
    "        else score_fn(str(row[\"text\"])),\n",
    "        axis=1,\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "\n",
    "def add_perspective_to_bundle(\n",
    "    bundle: Sequence,\n",
    "    score_fn: Callable[[str], Optional[dict]],\n",
    "    existing_bundle: Optional[Sequence] = None,\n",
    "    limit: Optional[int] = None,\n",
    "):\n",
    "    limit = len(bundle) if limit is None else min(limit, len(bundle))\n",
    "    scored: List = []\n",
    "    for idx in range(limit):\n",
    "        convo = bundle[idx]\n",
    "        prev_convo = existing_bundle[idx] if existing_bundle is not None and idx < len(existing_bundle) else None\n",
    "        if isinstance(convo, pd.DataFrame):\n",
    "            prev_frame = prev_convo if isinstance(prev_convo, pd.DataFrame) else None\n",
    "            scored.append(attach_perspective(convo, score_fn, prev_frame))\n",
    "        else:\n",
    "            prev_frames = prev_convo if isinstance(prev_convo, (list, tuple)) else []\n",
    "            frames = []\n",
    "            for j, frame in enumerate(convo):\n",
    "                prev_frame = (\n",
    "                    prev_frames[j]\n",
    "                    if j < len(prev_frames) and isinstance(prev_frames[j], pd.DataFrame)\n",
    "                    else None\n",
    "                )\n",
    "                frames.append(attach_perspective(frame, score_fn, prev_frame))\n",
    "            scored.append(frames)\n",
    "    return scored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee7831",
   "metadata": {},
   "source": [
    "\n",
    "### Smoke test on a small slice\n",
    "Keeps runtime short; flip `DRY_RUN = False` and `BATCH_LIMIT = None` for a full API-backed run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334d8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fn = make_score_fn(dry_run=DRY_RUN, api_key=PERSPECTIVE_API_KEY)\n",
    "scored_subset = add_perspective_to_bundle(\n",
    "    imitation_bundle,\n",
    "    score_fn=score_fn,\n",
    "    existing_bundle=existing_perspective,\n",
    "    limit=BATCH_LIMIT,\n",
    ")\n",
    "subset_flat = flatten_conversation_bundles(scored_subset)\n",
    "subset_flat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7ece10",
   "metadata": {},
   "source": [
    "\n",
    "### Full run (long; writes `OUTPUT_PATH`)\n",
    "Uncomment to score all conversations. Respects `DRY_RUN` and will overwrite `OUTPUT_PATH` with the newest run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_score_fn = make_score_fn(dry_run=DRY_RUN, api_key=PERSPECTIVE_API_KEY)\n",
    "# scored_bundle = add_perspective_to_bundle(\n",
    "#     imitation_bundle,\n",
    "#     score_fn=full_score_fn,\n",
    "#     existing_bundle=existing_perspective,\n",
    "# )\n",
    "# with OUTPUT_PATH.open(\"wb\") as fp:\n",
    "#     pickle.dump(scored_bundle, fp)\n",
    "# OUTPUT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440afc4",
   "metadata": {},
   "source": [
    "\n",
    "### Sanity checks and flat score view\n",
    "Inspect the saved bundle (or the subset if you have not run a full job) and derive per-attribute score columns for quick joins.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e38007",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_for_checks = None\n",
    "if OUTPUT_PATH.exists():\n",
    "    bundle_for_checks = load_df_list_pickle(OUTPUT_PATH)\n",
    "elif existing_perspective is not None:\n",
    "    bundle_for_checks = existing_perspective\n",
    "else:\n",
    "    bundle_for_checks = scored_subset\n",
    "\n",
    "checks_flat = flatten_conversation_bundles(bundle_for_checks)\n",
    "print(\"rows\", len(checks_flat))\n",
    "print(checks_flat[\"perspective\"].apply(lambda x: type(x).__name__).value_counts().head())\n",
    "\n",
    "\n",
    "def extract_summary_scores(entry: Optional[dict], attributes: Sequence[str] = PERSPECTIVE_ATTRIBUTES) -> Dict[str, Optional[float]]:\n",
    "    scores: Dict[str, Optional[float]] = {}\n",
    "    for attr in attributes:\n",
    "        value = None\n",
    "        if isinstance(entry, dict):\n",
    "            value = entry.get(attr, {}).get(\"summaryScore\", {}).get(\"value\")\n",
    "        scores[f\"persp_{attr.lower()}\"] = value\n",
    "    span_end = None\n",
    "    if isinstance(entry, dict):\n",
    "        spans = entry.get(attributes[0], {}).get(\"spanScores\", [])\n",
    "        if spans:\n",
    "            span_end = spans[0].get(\"end\")\n",
    "    scores[\"persp_span_end\"] = span_end\n",
    "    return scores\n",
    "\n",
    "score_frame = pd.DataFrame(checks_flat[\"perspective\"].apply(extract_summary_scores).tolist())\n",
    "flat_with_scores = pd.concat([checks_flat.drop(columns=[\"perspective\"]), score_frame], axis=1)\n",
    "flat_with_scores.head()\n",
    "\n",
    "# Optionally persist the flattened scores for downstream notebooks\n",
    "# flat_with_scores.to_parquet(FLAT_SCORES_PATH, index=False)\n",
    "# FLAT_SCORES_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

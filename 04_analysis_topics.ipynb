{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68784830",
   "metadata": {},
   "source": [
    "# 04 - Topic Analysis (Refusal vs Acceptance)\n",
    "Compare refusal vs acceptance corpora with bag-of-words/LDA and reuse migrated topic assets to avoid recomputing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead947d",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "- Load Perspective-scored imitation bundle and attach refusal markers.\n",
    "- Split per-conversation documents into refusal vs acceptance buckets.\n",
    "- Reuse migrated LDA runs (Count/TF-IDF) and include a small rerun hook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84f4c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from utils.data_io import load_df_list_pickle, flatten_conversation_bundles, describe_bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c496de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and toggles\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "ASSETS_PROCESSED = PROJECT_ROOT / \"assets\" / \"processed\"\n",
    "ASSETS_TOPICS = ASSETS_PROCESSED / \"topics\"\n",
    "\n",
    "PERSPECTIVE_PATH = ASSETS_PROCESSED / \"combat_threads_with_perspective.pkl\"\n",
    "\n",
    "LDA_TFIDF_REF = ASSETS_TOPICS / \"lda_results_tfidf_ref.pkl\"\n",
    "LDA_TFIDF_ACC = ASSETS_TOPICS / \"lda_results_tfidf_acc.pkl\"\n",
    "LDA_COUNT_REF = ASSETS_TOPICS / \"lda_results_count_ref.pkl\"\n",
    "LDA_COUNT_ACC = ASSETS_TOPICS / \"lda_results_count_acc.pkl\"\n",
    "LDA_MISC = [\n",
    "    ASSETS_TOPICS / \"lda_results_tfidf.pkl\",\n",
    "    ASSETS_TOPICS / \"lda_results_counter.pkl\",\n",
    "    ASSETS_TOPICS / \"lda_results_updated.pkl\",\n",
    "]\n",
    "\n",
    "REFUSAL_THRESHOLD = 0.1  # mark a conversation as refusal-heavy when > 10% turns refused\n",
    "TEXT_COLUMN = \"text\"    # default to original user utterances; switch to \"imm_1\" to analyze imitation output\n",
    "RUN_LDA = False          # set True to recompute a small LDA sweep locally\n",
    "N_TOP_WORDS = 12\n",
    "STOP_WORDS = [\"gt\", \"people\"]\n",
    "\n",
    "ASSETS_TOPICS.mkdir(parents=True, exist_ok=True)\n",
    "ASSETS_PROCESSED, ASSETS_TOPICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd8b8ce",
   "metadata": {},
   "source": [
    "### Asset manifest\n",
    "List the perspective bundle plus migrated topic assets (copied from `../Raja/revised_convo/`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e54c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = [\n",
    "    {\n",
    "        \"role\": \"input\",\n",
    "        \"path\": PERSPECTIVE_PATH,\n",
    "        \"note\": \"Imitation + Perspective bundle (list of DataFrames).\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"resume_optional\",\n",
    "        \"path\": LDA_TFIDF_REF,\n",
    "        \"note\": \"TF-IDF LDA on refusal-heavy conversations (source: Raja/revised_convo/lda_results_tfidf_ref.pkl).\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"resume_optional\",\n",
    "        \"path\": LDA_TFIDF_ACC,\n",
    "        \"note\": \"TF-IDF LDA on accepted conversations (source: Raja/revised_convo/lda_results_tfidf_acc.pkl).\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"resume_optional\",\n",
    "        \"path\": LDA_COUNT_REF,\n",
    "        \"note\": \"Count-vector LDA on refusal-heavy conversations (source: Raja/revised_convo/lda_results_count_ref.pkl).\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"resume_optional\",\n",
    "        \"path\": LDA_COUNT_ACC,\n",
    "        \"note\": \"Count-vector LDA on accepted conversations (source: Raja/revised_convo/lda_results_count_acc.pkl).\",\n",
    "    },\n",
    "]\n",
    "for extra in LDA_MISC:\n",
    "    manifest.append(\n",
    "        {\n",
    "            \"role\": \"archive_optional\",\n",
    "            \"path\": extra,\n",
    "            \"note\": f\"Additional LDA sweep ({extra.name}) migrated from Raja/revised_convo.\",\n",
    "        }\n",
    "    )\n",
    "manifest_df = pd.DataFrame(manifest)\n",
    "manifest_df[\"exists\"] = manifest_df[\"path\"].apply(Path.exists)\n",
    "manifest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860a48bb",
   "metadata": {},
   "source": [
    "### Load and label the perspective bundle\n",
    "Flatten into a DataFrame, attach a refusal flag (`is_refusal = not imm_1_check`), and keep a text column ready for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0be6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_bundle = load_df_list_pickle(PERSPECTIVE_PATH)\n",
    "print(\"bundle summary:\", describe_bundle(imitation_bundle))\n",
    "\n",
    "flat = flatten_conversation_bundles(imitation_bundle)\n",
    "flat[\"is_refusal\"] = ~flat[\"imm_1_check\"].astype(bool)\n",
    "flat[\"text_for_topics\"] = flat[TEXT_COLUMN].fillna(\"\").astype(str)\n",
    "\n",
    "print(\"rows:\", len(flat))\n",
    "print(flat[\"is_refusal\"].value_counts(normalize=True))\n",
    "flat.sample(5, random_state=0)[[\"conversation_idx\", \"text\", \"imm_1\", \"imm_1_check\", \"is_refusal\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99beaedd",
   "metadata": {},
   "source": [
    "### Conversation-level documents\n",
    "Collapse each conversation into a single document for topic modeling and mark refusal-heavy conversations using `REFUSAL_THRESHOLD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb522579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conversation_docs(df: pd.DataFrame, text_col: str, refusal_threshold: float) -> pd.DataFrame:\n",
    "    grouped = (\n",
    "        df.groupby(\"conversation_idx\")\n",
    "        .agg(\n",
    "            doc=(text_col, lambda s: \" \".join(s.dropna().astype(str))),\n",
    "            refusal_rate=(\"is_refusal\", \"mean\"),\n",
    "            row_count=(\"is_refusal\", \"size\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped[\"is_refusal_conversation\"] = grouped[\"refusal_rate\"] > refusal_threshold\n",
    "    return grouped\n",
    "\n",
    "conversation_docs = build_conversation_docs(flat, text_col=\"text_for_topics\", refusal_threshold=REFUSAL_THRESHOLD)\n",
    "print(conversation_docs[\"is_refusal_conversation\"].value_counts())\n",
    "conversation_docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29230fe3",
   "metadata": {},
   "source": [
    "### Corpus splits (refusal vs acceptance)\n",
    "These lists feed into the precomputed LDA runs; swap `TEXT_COLUMN` to switch between model outputs and original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91268059",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_refusal = conversation_docs.loc[conversation_docs[\"is_refusal_conversation\"], \"doc\"].tolist()\n",
    "docs_accept = conversation_docs.loc[~conversation_docs[\"is_refusal_conversation\"], \"doc\"].tolist()\n",
    "\n",
    "print(f\"refusal docs: {len(docs_refusal)} | accept docs: {len(docs_accept)}\")\n",
    "{\n",
    "    \"refusal_example\": docs_refusal[0][:200] if docs_refusal else \"\",\n",
    "    \"accept_example\": docs_accept[0][:200] if docs_accept else \"\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3add295",
   "metadata": {},
   "source": [
    "### Optional: run LDA locally (heavy)\n",
    "Keeps API-compatible outputs with the migrated pickles. Flip `RUN_LDA = True` and adjust `topic_range` to re-fit with Count/TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12956b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_words(model: LatentDirichletAllocation, vectorizer: Union[CountVectorizer, TfidfVectorizer], top_n: int) -> List[List[str]]:\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    topics: List[List[str]] = []\n",
    "    for topic in model.components_:\n",
    "        top_ids = topic.argsort()[:-top_n - 1:-1]\n",
    "        topics.append([feature_names[i] for i in top_ids])\n",
    "    return topics\n",
    "\n",
    "\n",
    "def run_lda_sweep(\n",
    "    docs: Sequence[str],\n",
    "    vectorizer: Union[CountVectorizer, TfidfVectorizer],\n",
    "    topic_counts: Sequence[int],\n",
    "    top_words: int,\n",
    "    random_state: int = 0,\n",
    ") -> List[Dict]:\n",
    "    results: List[Dict] = []\n",
    "    matrix = vectorizer.fit_transform(docs)\n",
    "    for n_topics in topic_counts:\n",
    "        lda = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            learning_method=\"batch\",\n",
    "            max_iter=30,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        lda.fit(matrix)\n",
    "        results.append(\n",
    "            {\n",
    "                \"n_topics\": n_topics,\n",
    "                \"model\": lda,\n",
    "                \"topics\": extract_top_words(lda, vectorizer, top_n=top_words),\n",
    "                \"perplexity\": lda.perplexity(matrix),\n",
    "                \"coherence\": None,  # placeholder to stay compatible with migrated pickles\n",
    "                \"vectorizer\": vectorizer,\n",
    "            }\n",
    "        )\n",
    "    return results\n",
    "\n",
    "\n",
    "topic_range = [5, 8, 10]\n",
    "if RUN_LDA:\n",
    "    tfidf_results_ref = run_lda_sweep(\n",
    "        docs_refusal,\n",
    "        TfidfVectorizer(stop_words=STOP_WORDS),\n",
    "        topic_counts=topic_range,\n",
    "        top_words=N_TOP_WORDS,\n",
    "    )\n",
    "    with (ASSETS_TOPICS / \"lda_results_tfidf_ref_repro.pkl\").open(\"wb\") as fp:\n",
    "        pickle.dump(tfidf_results_ref, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f5660",
   "metadata": {},
   "source": [
    "### Load migrated LDA results and summarize\n",
    "Quick glance at topic counts, vectorizers, and sample terms from each stored run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lda_results(path: Path) -> Optional[List[Dict]]:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    with path.open(\"rb\") as fp:\n",
    "        return pickle.load(fp)\n",
    "\n",
    "\n",
    "loaded_results: Dict[str, Optional[List[Dict]]] = {\n",
    "    \"tfidf_ref\": load_lda_results(LDA_TFIDF_REF),\n",
    "    \"tfidf_acc\": load_lda_results(LDA_TFIDF_ACC),\n",
    "    \"count_ref\": load_lda_results(LDA_COUNT_REF),\n",
    "    \"count_acc\": load_lda_results(LDA_COUNT_ACC),\n",
    "}\n",
    "for extra in LDA_MISC:\n",
    "    loaded_results[extra.stem] = load_lda_results(extra)\n",
    "\n",
    "summary_rows = []\n",
    "for name, runs in loaded_results.items():\n",
    "    if not runs:\n",
    "        continue\n",
    "    for entry in runs:\n",
    "        summary_rows.append(\n",
    "            {\n",
    "                \"asset\": name,\n",
    "                \"n_topics\": entry.get(\"n_topics\"),\n",
    "                \"vectorizer\": entry.get(\"vectorizer\").__class__.__name__ if entry.get(\"vectorizer\") else None,\n",
    "                \"perplexity\": entry.get(\"perplexity\"),\n",
    "                \"coherence\": entry.get(\"coherence\"),\n",
    "                \"sample_terms\": \", \".join(entry.get(\"topics\", [[]])[0][:5]) if entry.get(\"topics\") else \"\",\n",
    "            }\n",
    "        )\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_df.sort_values([\"asset\", \"n_topics\"]).head(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb46c14",
   "metadata": {},
   "source": [
    "### Preview topics from the best runs\n",
    "Pick the highest-coherence run per split and show its top terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecee8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_best(runs: Optional[List[Dict]]) -> Optional[Dict]:\n",
    "    if not runs:\n",
    "        return None\n",
    "    return max(\n",
    "        runs,\n",
    "        key=lambda r: (\n",
    "            r.get(\"coherence\") if r.get(\"coherence\") is not None else -1,\n",
    "            -(r.get(\"perplexity\") or 0),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def topics_frame(entry: Dict) -> pd.DataFrame:\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"topic\": range(len(entry.get(\"topics\", []))),\n",
    "            \"top_terms\": [\" \".join(words) for words in entry.get(\"topics\", [])],\n",
    "            \"n_topics\": entry.get(\"n_topics\"),\n",
    "            \"vectorizer\": entry.get(\"vectorizer\").__class__.__name__ if entry.get(\"vectorizer\") else None,\n",
    "        }\n",
    "    )\n",
    "\n",
    "best_ref = choose_best(loaded_results.get(\"tfidf_ref\"))\n",
    "best_acc = choose_best(loaded_results.get(\"tfidf_acc\"))\n",
    "\n",
    "frames = []\n",
    "if best_ref:\n",
    "    frames.append(topics_frame(best_ref).assign(split=\"refusal\"))\n",
    "if best_acc:\n",
    "    frames.append(topics_frame(best_acc).assign(split=\"accept\"))\n",
    "\n",
    "pd.concat(frames, ignore_index=True) if frames else \"No LDA assets loaded.\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

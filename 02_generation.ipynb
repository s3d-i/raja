{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d39c14d",
   "metadata": {},
   "source": [
    "\n",
    "# 02 - Generation (Imitation / Summarization)\n",
    "\n",
    "Produce LLM imitations/summaries for each combat turn to feed Perspective + topic analysis. Keep paths explicit to make reruns reproducible and skip already generated rows when possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec92ec",
   "metadata": {},
   "source": [
    "\n",
    "**Goals**\n",
    "- Load conversation bundles from `assets/raw/` and reuse any seeded summaries when present.\n",
    "- Define a prompt + local LLM connector to generate `imm_1` and refusal flag `imm_1_check`.\n",
    "- Persist the regenerated bundle to `assets/processed/` with quick sanity checks before later notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea114faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from typing import Callable, Iterable, List, Optional, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from utils.data_io import load_df_list_pickle, flatten_conversation_bundles, describe_bundle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead80dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Paths and run settings\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "ASSETS_RAW = PROJECT_ROOT / \"assets\" / \"raw\"\n",
    "ASSETS_PROCESSED = PROJECT_ROOT / \"assets\" / \"processed\"\n",
    "\n",
    "RAW_COMBAT = ASSETS_RAW / \"combat_threads_text_only.pkl\"\n",
    "AGU_SEED = ASSETS_RAW / \"combat_threads_with_agu_sample.pkl\"  # optional helper\n",
    "PREVIOUS_IMM = ASSETS_PROCESSED / \"combat_threads_with_imitation.pkl\"\n",
    "OUTPUT_PATH = ASSETS_PROCESSED / \"combat_threads_with_imitation_regen.pkl\"\n",
    "\n",
    "MODEL_NAME = \"llama3\"\n",
    "OLLAMA_ENDPOINT = \"http://localhost:11434/api/generate\"\n",
    "DRY_RUN = True  # flip to False to hit a running local model\n",
    "\n",
    "ASSETS_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "RAW_COMBAT, AGU_SEED, PREVIOUS_IMM, OUTPUT_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4de8b5a",
   "metadata": {},
   "source": [
    "\n",
    "### Asset manifest\n",
    "Confirm which inputs are available and where the regenerated bundle will land.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aab38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "asset_manifest = [\n",
    "    {\n",
    "        \"role\": \"input\",\n",
    "        \"path\": RAW_COMBAT,\n",
    "        \"note\": \"6842 combat conversations; text only extracted from convokit reply chains.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"input_optional\",\n",
    "        \"path\": AGU_SEED,\n",
    "        \"note\": \"334-sample with agu_1 summarization; useful for prompt shaping only.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"resume_optional\",\n",
    "        \"path\": PREVIOUS_IMM,\n",
    "        \"note\": \"Existing imitation run (imm_1 + imm_1_check) that can be reused or compared.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"output\",\n",
    "        \"path\": OUTPUT_PATH,\n",
    "        \"note\": \"Regenerated imitation bundle with imm_1 + imm_1_check.\",\n",
    "    },\n",
    "]\n",
    "manifest_df = pd.DataFrame(asset_manifest)\n",
    "manifest_df[\"exists\"] = manifest_df[\"path\"].apply(lambda p: p.exists())\n",
    "manifest_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518cdd2",
   "metadata": {},
   "source": [
    "\n",
    "### Inspect source bundle\n",
    "Lightweight stats so generation logic can stay defensive against the list-of-DataFrames layout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b1c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_bundle = load_df_list_pickle(RAW_COMBAT)\n",
    "print(\"raw bundle:\", describe_bundle(raw_bundle))\n",
    "raw_preview = flatten_conversation_bundles(raw_bundle[:1])\n",
    "display(raw_preview.head())\n",
    "\n",
    "agu_bundle_present = AGU_SEED.exists()\n",
    "if agu_bundle_present:\n",
    "    agu_bundle = load_df_list_pickle(AGU_SEED)\n",
    "    print(\"agu sample:\", describe_bundle(agu_bundle))\n",
    "    display(flatten_conversation_bundles(agu_bundle[:1]).head())\n",
    "else:\n",
    "    agu_bundle = None\n",
    "\n",
    "existing_bundle = load_df_list_pickle(PREVIOUS_IMM) if PREVIOUS_IMM.exists() else None\n",
    "if existing_bundle is not None:\n",
    "    print(\"existing imm:\", describe_bundle(existing_bundle))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae46481",
   "metadata": {},
   "source": [
    "### Prompt and refusal heuristics\n",
    "Replicates the pre-clean pipeline: enforce the JSON `{\"--IMMITATION--\": ...}` output format and treat the presence of the marker as success.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d569ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = (\n",
    "'OUTPUT FORMAT:{\"--IMMITATION--\": YOUR_IMMITATION_TEXTS}'\n",
    "\"Your immitation should preserve the Speaker's semantic meaning and emotions sentence by sentence.\"\n",
    ")\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"Speaker's Text:{comment}\"\n",
    "IMMITATION_MARKER = \"--IMMITATION--\"\n",
    "POSSIBLE_SIGNAL = '\": '\n",
    "\n",
    "\n",
    "def build_prompt(comment: str) -> str:\n",
    "    return f\"{SYSTEM_PROMPT}\" + USER_PROMPT_TEMPLATE.format(comment=comment)\n",
    "\n",
    "\n",
    "def run_local_ollama(prompt: str, model: str = MODEL_NAME, endpoint: str = OLLAMA_ENDPOINT) -> str:\n",
    "    payload = {\"model\": model, \"prompt\": prompt, \"stream\": False}\n",
    "    response = requests.post(endpoint, json=payload, timeout=120)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return data.get(\"response\", \"\").strip()\n",
    "\n",
    "\n",
    "def parse_immitation_output(text: str, original: str) -> tuple[str, bool]:\n",
    "    \"\"\"Return (immitation_text, success_flag) mirroring the pre-clean pipeline.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return original, False\n",
    "    if IMMITATION_MARKER not in text:\n",
    "        return original, False\n",
    "    result = text.split(IMMITATION_MARKER, 1)[1]\n",
    "    if \"}\" in result:\n",
    "        result = result.split(\"}\", 1)[0]\n",
    "    if POSSIBLE_SIGNAL in result:\n",
    "        parts = result.split(POSSIBLE_SIGNAL)\n",
    "        result = parts[1] if len(parts) > 1 else parts[0]\n",
    "    return result.strip(), True\n",
    "\n",
    "\n",
    "def mark_refusal(text: str) -> bool:\n",
    "    _, ok = parse_immitation_output(text, original=\"\")\n",
    "    return not ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10837815",
   "metadata": {},
   "source": [
    "\n",
    "### Generation helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b6ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_frame(frame: pd.DataFrame, existing: Optional[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Attach existing generation columns (if available) before re-processing.\"\"\"\n",
    "    base = frame.copy()\n",
    "    if existing is not None:\n",
    "        available_cols = [col for col in [\"imm_1\", \"imm_1_check\"] if col in existing.columns]\n",
    "        base = base.join(existing[available_cols], how=\"left\")\n",
    "    return base\n",
    "\n",
    "\n",
    "def attach_generation(frame: pd.DataFrame, run_fn: Callable[[str], str], dry_run: bool = True) -> pd.DataFrame:\n",
    "    df = frame.copy()\n",
    "\n",
    "    def _generate(row):\n",
    "        # Keep prior generation when present to avoid redundant calls.\n",
    "        if isinstance(row.get(\"imm_1\"), str) and row[\"imm_1\"].strip():\n",
    "            return row[\"imm_1\"]\n",
    "        if dry_run:\n",
    "            return f'{{\"{IMMITATION_MARKER}\": \"{row[\"text\"][:200]}\"}}'\n",
    "        prompt = build_prompt(row[\"text\"])\n",
    "        return run_fn(prompt)\n",
    "\n",
    "    df[\"imm_1\"] = df.apply(_generate, axis=1)\n",
    "\n",
    "    def _keep_or_parse(row):\n",
    "        if \"imm_1_check\" in row and pd.notna(row[\"imm_1_check\"]):\n",
    "            return row[\"imm_1\"], row[\"imm_1_check\"]\n",
    "        parsed_text, ok = parse_immitation_output(row[\"imm_1\"], row[\"text\"])\n",
    "        return parsed_text, ok\n",
    "\n",
    "    df[\"imm_1\"], df[\"imm_1_check\"] = zip(*df.apply(_keep_or_parse, axis=1))\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_bundle(\n",
    "    bundles: Sequence,\n",
    "    run_fn: Callable[[str], str],\n",
    "    dry_run: bool = True,\n",
    "    existing_bundle: Optional[Sequence] = None,\n",
    "    limit_conversations: Optional[int] = None,\n",
    ") -> List:\n",
    "    output: List = []\n",
    "    for convo_idx, convo in enumerate(bundles):\n",
    "        if limit_conversations is not None and convo_idx >= limit_conversations:\n",
    "            break\n",
    "\n",
    "        frames = convo if isinstance(convo, (list, tuple)) else [convo]\n",
    "        existing_frames = None\n",
    "        if existing_bundle is not None and convo_idx < len(existing_bundle):\n",
    "            candidate = existing_bundle[convo_idx]\n",
    "            existing_frames = candidate if isinstance(candidate, (list, tuple)) else [candidate]\n",
    "\n",
    "        processed_frames: List = []\n",
    "        for frame_idx, frame in enumerate(frames):\n",
    "            existing_frame = None\n",
    "            if existing_frames is not None and frame_idx < len(existing_frames):\n",
    "                existing_frame = existing_frames[frame_idx]\n",
    "            prepared = prepare_frame(frame, existing_frame)\n",
    "            processed_frames.append(attach_generation(prepared, run_fn, dry_run=dry_run))\n",
    "\n",
    "        output.append(processed_frames if isinstance(convo, (list, tuple)) else processed_frames[0])\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c41b6f3",
   "metadata": {},
   "source": [
    "\n",
    "### Smoke test on a small subset\n",
    "Set `DRY_RUN = False` to hit the local model; `limit_conversations` keeps early debugging cheap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699913e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subset_existing = existing_bundle[:2] if existing_bundle is not None else None\n",
    "subset_generated = generate_bundle(\n",
    "    raw_bundle[:2],\n",
    "    run_fn=run_local_ollama,\n",
    "    dry_run=DRY_RUN,\n",
    "    existing_bundle=subset_existing,\n",
    ")\n",
    "flattened_subset = flatten_conversation_bundles(subset_generated)\n",
    "flattened_subset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51692fff",
   "metadata": {},
   "source": [
    "\n",
    "### Full run (long; generates `OUTPUT_PATH`)\n",
    "Uncomment the following cell to generate all conversations and persist the bundle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# full_generated = generate_bundle(\n",
    "#     raw_bundle,\n",
    "#     run_fn=run_local_ollama,\n",
    "#     dry_run=DRY_RUN,\n",
    "#     existing_bundle=existing_bundle,\n",
    "# )\n",
    "# with OUTPUT_PATH.open(\"wb\") as fp:\n",
    "#     pickle.dump(full_generated, fp)\n",
    "# OUTPUT_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487682cb",
   "metadata": {},
   "source": [
    "\n",
    "### Sanity checks on saved output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if OUTPUT_PATH.exists():\n",
    "    regen_bundle = load_df_list_pickle(OUTPUT_PATH)\n",
    "    regen_flat = flatten_conversation_bundles(regen_bundle)\n",
    "    print(\"rows\", len(regen_flat))\n",
    "    print(regen_flat[\"imm_1_check\"].value_counts(dropna=False).head())\n",
    "    display(regen_flat.sample(5, random_state=0)[[\"text\", \"imm_1\", \"imm_1_check\"]])\n",
    "else:\n",
    "    print(\"No regenerated bundle found; run the full generation cell above first.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

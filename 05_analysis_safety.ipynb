{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f00422",
   "metadata": {},
   "source": [
    "\n",
    "# 05 - Safety & Refusal Analysis\n",
    "\n",
    "Inspect refusal behavior and Perspective safety scores to understand how imitation outputs differ across toxic and non-toxic cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f527979",
   "metadata": {},
   "source": [
    "\n",
    "**Goals**\n",
    "- Load the processed imitation + Perspective bundle (and optional variants) for safety analysis.\n",
    "- Flatten Perspective attributes into analyzable columns alongside refusal flags.\n",
    "- Summarize refusal rates, compare safety attributes, and surface high-toxicity examples for manual review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.data_io import load_df_list_pickle, flatten_conversation_bundles, describe_bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c88b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and toggles\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "ASSETS_PROCESSED = PROJECT_ROOT / \"assets\" / \"processed\"\n",
    "\n",
    "PERSPECTIVE_PATH = ASSETS_PROCESSED / \"combat_threads_with_perspective.pkl\"\n",
    "PERSPECTIVE_LIST_PATH = ASSETS_PROCESSED / \"combat_threads_with_perspective_list.pkl\"\n",
    "PERSPECTIVE_MORE_REFUSE_PATH = ASSETS_PROCESSED / \"combat_threads_with_perspective_list_more_refuse_cleaned.pkl\"\n",
    "FLAT_SCORES_PATH = ASSETS_PROCESSED / \"combat_threads_with_perspective_scores.parquet\"\n",
    "\n",
    "SOURCE_MODE = \"perspective_list\"  # options: perspective_list, more_refuse, base\n",
    "HIGH_TOXICITY = 0.5  # label a turn as toxic when Perspective TOXICITY >= this\n",
    "REFUSAL_CONVERSATION_THRESHOLD = 0.1  # used when bucketing conversations by refusal share\n",
    "REFUSAL_TRUE_RATE_THRESHOLD = 0.67  # more_refuse bundle marks refusal when true_rate falls below this share\n",
    "\n",
    "ASSETS_PROCESSED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c06f8f",
   "metadata": {},
   "source": [
    "\n",
    "### Asset manifest\n",
    "List available inputs/variants and the derived artifact this notebook can emit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_source(mode: str):\n",
    "    if mode == \"more_refuse\" and PERSPECTIVE_MORE_REFUSE_PATH.exists():\n",
    "        return PERSPECTIVE_MORE_REFUSE_PATH\n",
    "    if mode == \"perspective_list\" and PERSPECTIVE_LIST_PATH.exists():\n",
    "        return PERSPECTIVE_LIST_PATH\n",
    "    return PERSPECTIVE_PATH\n",
    "\n",
    "\n",
    "SOURCE_PATH = resolve_source(SOURCE_MODE)\n",
    "manifest = [\n",
    "    {\n",
    "        \"role\": \"source\",\n",
    "        \"path\": SOURCE_PATH,\n",
    "        \"note\": \"Bundle used for analysis (selected via SOURCE_MODE).\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"variant_optional\",\n",
    "        \"path\": PERSPECTIVE_LIST_PATH,\n",
    "        \"note\": \"Perspective dict + pre-extracted list vectors (default source when present).\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"variant_optional\",\n",
    "        \"path\": PERSPECTIVE_MORE_REFUSE_PATH,\n",
    "        \"note\": \"Same as above with extra refusal heuristics (refuse_add/true_rate).\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"fallback\",\n",
    "        \"path\": PERSPECTIVE_PATH,\n",
    "        \"note\": \"Base Perspective dicts without flattened score vectors.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"output_optional\",\n",
    "        \"path\": FLAT_SCORES_PATH,\n",
    "        \"note\": \"Optional parquet of flattened Perspective scores for downstream notebooks.\",\n",
    "    },\n",
    "]\n",
    "manifest_df = pd.DataFrame(manifest).drop_duplicates(subset=[\"path\"])\n",
    "manifest_df[\"exists\"] = manifest_df[\"path\"].apply(lambda p: Path(p).exists())\n",
    "manifest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1baa83",
   "metadata": {},
   "source": [
    "\n",
    "### Load and flatten bundle\n",
    "Load the selected bundle, preserve conversation grouping, and add a binary refusal flag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SOURCE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Bundle not found: {SOURCE_PATH}\")\n",
    "\n",
    "bundle = load_df_list_pickle(SOURCE_PATH)\n",
    "print(f\"using source: {SOURCE_PATH.name}\")\n",
    "print(\"bundle summary:\", describe_bundle(bundle))\n",
    "\n",
    "flat = flatten_conversation_bundles(bundle)\n",
    "if SOURCE_MODE == \"more_refuse\" and \"true_rate\" in flat.columns:\n",
    "    flat[\"is_refusal\"] = flat[\"true_rate\"].fillna(0) < REFUSAL_TRUE_RATE_THRESHOLD\n",
    "    print(\n",
    "        f\"refusal derived from true_rate<{REFUSAL_TRUE_RATE_THRESHOLD} \"\n",
    "        f\"({flat['is_refusal'].sum()} flagged)\"\n",
    "    )\n",
    "else:\n",
    "    flat[\"is_refusal\"] = ~flat[\"imm_1_check\"].astype(bool)\n",
    "    print(f\"refusal derived from imm_1_check ({flat['is_refusal'].sum()} flagged)\")\n",
    "print(\"rows\", len(flat))\n",
    "print(\"columns\", flat.columns.tolist())\n",
    "flat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3cabd3",
   "metadata": {},
   "source": [
    "\n",
    "### Perspective score extraction\n",
    "Normalize Perspective output into per-attribute columns. If `perspective_ls` is present it is expanded; otherwise scores are pulled from the raw Perspective dicts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4942bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PERSPECTIVE_ATTRIBUTES = [\n",
    "    \"AFFINITY_EXPERIMENTAL\",\n",
    "    \"COMPASSION_EXPERIMENTAL\",\n",
    "    \"CURIOSITY_EXPERIMENTAL\",\n",
    "    \"IDENTITY_ATTACK\",\n",
    "    \"IDENTITY_ATTACK_EXPERIMENTAL\",\n",
    "    \"INSULT\",\n",
    "    \"INSULT_EXPERIMENTAL\",\n",
    "    \"NUANCE_EXPERIMENTAL\",\n",
    "    \"PERSONAL_STORY_EXPERIMENTAL\",\n",
    "    \"PROFANITY\",\n",
    "    \"PROFANITY_EXPERIMENTAL\",\n",
    "    \"REASONING_EXPERIMENTAL\",\n",
    "    \"RESPECT_EXPERIMENTAL\",\n",
    "    \"SEVERE_TOXICITY\",\n",
    "    \"SEVERE_TOXICITY_EXPERIMENTAL\",\n",
    "    \"SEXUALLY_EXPLICIT\",\n",
    "    \"THREAT\",\n",
    "    \"THREAT_EXPERIMENTAL\",\n",
    "    \"TOXICITY\",\n",
    "    \"TOXICITY_EXPERIMENTAL\",\n",
    "]\n",
    "\n",
    "\n",
    "def extract_summary_scores(entry: Optional[dict], attributes: Sequence[str] = PERSPECTIVE_ATTRIBUTES) -> Dict[str, Optional[float]]:\n",
    "    scores: Dict[str, Optional[float]] = {}\n",
    "    for attr in attributes:\n",
    "        key = f\"persp_{attr.lower()}\"\n",
    "        value = None\n",
    "        if isinstance(entry, dict):\n",
    "            value = entry.get(attr, {}).get(\"summaryScore\", {}).get(\"value\")\n",
    "        scores[key] = value\n",
    "    span_end = None\n",
    "    if isinstance(entry, dict):\n",
    "        spans = entry.get(attributes[0], {}).get(\"spanScores\", [])\n",
    "        if spans:\n",
    "            span_end = spans[0].get(\"end\")\n",
    "    scores[\"persp_span_end\"] = span_end\n",
    "    return scores\n",
    "\n",
    "\n",
    "def perspective_row_to_dict(row: pd.Series) -> Dict[str, Optional[float]]:\n",
    "    ls = row.get(\"perspective_ls\")\n",
    "    if isinstance(ls, (list, tuple)) and len(ls) >= len(PERSPECTIVE_ATTRIBUTES) + 1:\n",
    "        scores = {f\"persp_{attr.lower()}\": val for attr, val in zip(PERSPECTIVE_ATTRIBUTES, ls[1:])}\n",
    "        scores[\"persp_span_end\"] = ls[0]\n",
    "        return scores\n",
    "    return extract_summary_scores(row.get(\"perspective\"))\n",
    "\n",
    "\n",
    "flat_reset = flat.reset_index(drop=True)\n",
    "score_frame = pd.DataFrame(flat_reset.apply(perspective_row_to_dict, axis=1).tolist())\n",
    "analysis_df = pd.concat([flat_reset, score_frame], axis=1)\n",
    "score_columns = [c for c in analysis_df.columns if c.startswith(\"persp_\")]\n",
    "print(\"score columns\", score_columns[:5], \"...\", len(score_columns))\n",
    "analysis_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fef7ea",
   "metadata": {},
   "source": [
    "\n",
    "### Refusal overview\n",
    "Basic refusal rates at utterance and conversation levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f1139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "refusal_rate = analysis_df[\"is_refusal\"].mean()\n",
    "convo_refusal = analysis_df.groupby(\"conversation_idx\")[\"is_refusal\"].mean()\n",
    "refusal_convo_share = (convo_refusal > REFUSAL_CONVERSATION_THRESHOLD).mean()\n",
    "print({\"refusal_rate\": refusal_rate, \"refusal_convo_share\": refusal_convo_share})\n",
    "convo_refusal.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb13cdfb",
   "metadata": {},
   "source": [
    "\n",
    "### Perspective attribute comparison\n",
    "Contrast mean Perspective scores between refused vs accepted turns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d5d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score_cols = [c for c in analysis_df.columns if c.startswith(\"persp_\") and c != \"persp_span_end\"]\n",
    "means = analysis_df.groupby(\"is_refusal\")[score_cols].mean().T\n",
    "means = means.rename(columns={False: \"accept_mean\", True: \"refuse_mean\"})\n",
    "means[\"delta_refuse_minus_accept\"] = means[\"refuse_mean\"] - means[\"accept_mean\"]\n",
    "means.sort_values(\"delta_refuse_minus_accept\", ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b881ea",
   "metadata": {},
   "source": [
    "\n",
    "### Toxicity thresholds and examples\n",
    "Flag high-toxicity turns and surface examples for manual inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ea7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "analysis_df[\"is_toxic\"] = analysis_df[\"persp_toxicity\"].fillna(0) >= HIGH_TOXICITY\n",
    "ct = pd.crosstab(analysis_df[\"is_refusal\"], analysis_df[\"is_toxic\"], normalize=\"index\")\n",
    "print(ct)\n",
    "print(analysis_df[\"persp_toxicity\"].describe())\n",
    "\n",
    "refused_examples = (\n",
    "    analysis_df[analysis_df[\"is_refusal\"]]\n",
    "    .nlargest(5, \"persp_toxicity\")\n",
    "    [[\"conversation_idx\", \"text\", \"imm_1\", \"persp_toxicity\"]]\n",
    ")\n",
    "accepted_examples = (\n",
    "    analysis_df[~analysis_df[\"is_refusal\"]]\n",
    "    .nlargest(5, \"persp_toxicity\")\n",
    "    [[\"conversation_idx\", \"text\", \"imm_1\", \"persp_toxicity\"]]\n",
    ")\n",
    "refused_examples, accepted_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4104b486",
   "metadata": {},
   "source": [
    "\n",
    "### Optional export of flattened scores\n",
    "Uncomment to materialize a flat Perspective score table for downstream notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# analysis_df.to_parquet(FLAT_SCORES_PATH, index=False)\n",
    "# FLAT_SCORES_PATH\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

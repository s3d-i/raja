# Project Overview (cleaned version)

## Research goal
Extract A↔B combative sub-dialogues (≥3 turns) from the Reddit convokit corpus, generate LLM imitation/summaries, combine Perspective API scores and topic modeling, and analyze model refusal/safety behavior and topic differences.

## Directory layout
- `utils/`: Shared utilities (data IO, preprocessing, LLM imitation, Perspective calls, analysis/visualization).
- `assets/`: Data and model artifacts (split into `raw/`, `processed/`, `models/`; keep placeholders for very large files when needed).
- `assets/` is also published to Hugging Face at `https://huggingface.co/datasets/NyaaaaaQwQ/conversational-bundles/tree/main/assets`.
- Top-level notebooks (pipeline ordered):
  - `01_data_ingest.ipynb`: Read data, extract combats, basic cleaning → `assets/raw/`
  - `02_generation.ipynb`: LLM imitation/summary/conditioning → `assets/processed/`
  - `03_perspective.ipynb`: Perspective batch scoring and cleaning → `assets/processed/`
  - `04_analysis_topics.ipynb`: BoW/TF-IDF/Count/LDA comparison (refuse vs accept)
  - `05_analysis_safety.ipynb`: Refusal rate, toxicity distribution, case filtering, visualization
  - `06_report.ipynb`: Result consolidation and report visuals

## Existing assets (sources to track)
- `combat_df_list*.pkl`: Combative text, LLM summaries/imitations, and fields like `imm*_check`, `refuse`.
- `*_perspective*.pkl`: Perspective-scored variants (with different cleaning/filters).
- `lda_results_*.pkl`: Topic/vectorization outputs.
- `assets/processed/topics/lda_results_*.pkl`: TF-IDF/Count LDA results split by refuse/accept, migrated from `../Raja/revised_convo/`.
- `assets/processed/combat_threads_with_perspective*.pkl`: Migrated from `../Raja/revised_convo/combat_df_list_imms_1_full_perspective*.pkl`; includes imitation text with Perspective scores (`*_list` carries `perspective_ls`; `*_more_refuse_cleaned` adds `true_rate`/`refuse_add`).
- `assets/processed/report/`: Report snapshot output directory (generated by `06_report.ipynb`, empty by default).
- `jp_vs_cn.csv`: YouTube comment data (separate exploration).
- Others: `llama3-tokenizer.model`, model conversion scripts, etc.

## Dependencies (draft)
See `requirements.txt`. No remote API key required by default; uses local llama.cpp / Ollama services.

## Next steps
- Consolidate and document legacy pickles under `assets/`, noting generation steps and fields.
- Extract and reuse shared utilities under `utils/` to replace notebook duplication.
- In notebooks, log input/output paths, shapes, and artifacts in order for reproducibility.

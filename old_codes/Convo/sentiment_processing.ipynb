{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-06T23:57:59.532100Z",
     "start_time": "2024-08-06T23:57:53.153289Z"
    }
   },
   "source": [
    "import spacy\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# 加载SpaCy模型\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 初始化VADER情感分析器\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 定义情感分析函数\n",
    "def sentiment_analysis(text):\n",
    "    \n",
    "    # 使用VADER进行情感分析\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    # sample['meta.parsed'][0]\n",
    "    return sentiment\n",
    "\n",
    "# 示例文本\n",
    "text = \"I love natural language processing. It's fascinating and fun!\"\n",
    "\n",
    "# 进行情感分析\n",
    "result = sentiment_analysis(text)\n",
    "print(result)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.266, 'pos': 0.734, 'compound': 0.9299}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T23:58:25.109883Z",
     "start_time": "2024-08-06T23:57:59.533131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import spacy\n",
    "from gensim import corpora, models\n",
    "from convokit import Corpus, download\n",
    "\n",
    "# 加载spaCy模型\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "path = \"C:/Users/L/.convokit/downloads/\"\n",
    "# 加载对话语料库\n",
    "corpus = Corpus(filename=path + 'conversations-gone-awry-cmv-corpus')\n",
    "\n",
    "# 依存解析示例\n",
    "def analyze_dependency(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")\n",
    "\n",
    "# 主题建模示例\n",
    "def topic_modeling(texts):\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "    return lda_model.print_topics()\n",
    "\n",
    "# 分析对话中的攻击性语言\n",
    "def detect_aggression(text):\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'amod' and token.head.dep_ == 'ROOT':\n",
    "            print(f\"Aggressive language detected: {token.text} -> {token.head.text}\")\n",
    "\n",
    "\n",
    "\n",
    "# 示例文本\n",
    "texts = [[\"I\", \"love\", \"fuck\", \"natural\", \"language\", \"processing\"], [\"It's\", \"fascinating\", \"and\", \"fun\"]]\n",
    "text = \"I love natural language processing. It's fascinating and fun!\"\n",
    "\n",
    "# 依存解析\n",
    "analyze_dependency(text)\n",
    "\n",
    "# 主题建模\n",
    "print(topic_modeling(texts))\n",
    "\n",
    "# 检测攻击性语言\n",
    "detect_aggression(text)\n",
    "\n"
   ],
   "id": "536f6d125df130e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -> nsubj -> love\n",
      "love -> ROOT -> love\n",
      "natural -> amod -> language\n",
      "language -> compound -> processing\n",
      "processing -> dobj -> love\n",
      ". -> punct -> love\n",
      "It -> nsubj -> 's\n",
      "'s -> ROOT -> 's\n",
      "fascinating -> acomp -> 's\n",
      "and -> cc -> fascinating\n",
      "fun -> conj -> fascinating\n",
      "! -> punct -> 's\n",
      "[(0, '0.150*\"love\" + 0.150*\"processing\" + 0.150*\"natural\" + 0.150*\"fuck\" + 0.150*\"language\" + 0.150*\"I\" + 0.025*\"and\" + 0.025*\"fun\" + 0.025*\"It\\'s\" + 0.025*\"fascinating\"'), (1, '0.100*\"It\\'s\" + 0.100*\"I\" + 0.100*\"and\" + 0.100*\"fascinating\" + 0.100*\"fun\" + 0.100*\"language\" + 0.100*\"fuck\" + 0.100*\"natural\" + 0.100*\"processing\" + 0.100*\"love\"'), (2, '0.200*\"fascinating\" + 0.200*\"fun\" + 0.200*\"It\\'s\" + 0.200*\"and\" + 0.033*\"fuck\" + 0.033*\"language\" + 0.033*\"I\" + 0.033*\"natural\" + 0.033*\"processing\" + 0.033*\"love\"'), (3, '0.100*\"and\" + 0.100*\"fascinating\" + 0.100*\"It\\'s\" + 0.100*\"fun\" + 0.100*\"I\" + 0.100*\"fuck\" + 0.100*\"love\" + 0.100*\"natural\" + 0.100*\"processing\" + 0.100*\"language\"'), (4, '0.100*\"It\\'s\" + 0.100*\"and\" + 0.100*\"fun\" + 0.100*\"fascinating\" + 0.100*\"I\" + 0.100*\"fuck\" + 0.100*\"language\" + 0.100*\"processing\" + 0.100*\"natural\" + 0.100*\"love\"')]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T23:58:25.119514Z",
     "start_time": "2024-08-06T23:58:25.110396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os, pandas as pd\n",
    "from convokit import Corpus, Speaker, Utterance, Conversation, download\n",
    "\n",
    "path = \"C:/Users/L/.convokit/downloads/\"\n",
    "os.environ['http_proxy'] = 'http://localhost:7890'\n",
    "os.environ['https_proxy'] = 'http://localhost:7890'\n",
    "\n",
    "def load_dfs(corpus):\n",
    "    speakers = corpus.get_speakers_dataframe().drop(columns=['vectors'])\n",
    "    \n",
    "    conversations = corpus.get_conversations_dataframe().drop(columns=['vectors'])\n",
    "    utterances = corpus.get_utterances_dataframe().drop(columns=['vectors'])\n",
    "    # print(type(speakers), type(conversations), type(utterances))\n",
    "    return speakers, conversations, utterances\n",
    "\n",
    "def print_overview(speaker_df, convo_df, utt_df):\n",
    "    print(\"UttDf attributes:\", list(utt_df.columns),'\\n')\n",
    "    print(\"ConvDf attributes:\", list(convo_df.columns),'\\n')\n",
    "    print(\"SpeakerDf attributes:\", list(speaker_df.columns),'\\n')\n",
    "\n",
    "    # print(convo_df.sample(n=2))\n",
    "    print(\"convo:\",convo_df.shape)\n",
    "    # print(speaker_df.sample(n=2))\n",
    "    print(\"speaker:\",speaker_df.shape)\n",
    "    \n",
    "    print(\"utt:\",utt_df.shape)\n",
    "    # print(utt_df.sample(n=2))\n",
    "    \n",
    "def check_column_type(df):\n",
    "    r = df.sample(n=1)\n",
    "    for i in df.columns:\n",
    "        print(i, type(r[i].values[0]))\n",
    "        \n",
    "def save_df_pickle(df, path):\n",
    "    df.to_pickle(path)\n",
    "    print(\"Save to\", path)\n",
    "    \n",
    "def load_df_pickle(path):\n",
    "    df = pd.read_pickle(path)\n",
    "    return df\n"
   ],
   "id": "6843fc1350490b4d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T23:58:35.355859Z",
     "start_time": "2024-08-06T23:58:25.121517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus = Corpus(filename=path+'conversations-gone-awry-cmv-corpus')\n",
    "speakers, conversations, utterances = load_dfs(corpus)\n",
    "print_overview(speakers, conversations, utterances)\n",
    "check_column_type(utterances)"
   ],
   "id": "8876a439ec4cc4c2",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m corpus \u001B[38;5;241m=\u001B[39m \u001B[43mCorpus\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mconversations-gone-awry-cmv-corpus\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m speakers, conversations, utterances \u001B[38;5;241m=\u001B[39m load_dfs(corpus)\n\u001B[0;32m      3\u001B[0m print_overview(speakers, conversations, utterances)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raja\\Lib\\site-packages\\convokit\\model\\corpus.py:143\u001B[0m, in \u001B[0;36mCorpus.__init__\u001B[1;34m(self, filename, utterances, db_collection_prefix, db_host, preload_vectors, utterance_start_index, utterance_end_index, merge_lines, exclude_utterance_meta, exclude_conversation_meta, exclude_speaker_meta, exclude_overall_meta, disable_type_check, backend, backend_mapper)\u001B[0m\n\u001B[0;32m    141\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmeta_index\u001B[38;5;241m.\u001B[39mdisable_type_check()\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(filename):\n\u001B[1;32m--> 143\u001B[0m     utterances \u001B[38;5;241m=\u001B[39m \u001B[43mload_utterance_info_from_dir\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mutterance_start_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mutterance_end_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexclude_utterance_meta\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    147\u001B[0m     speakers_data \u001B[38;5;241m=\u001B[39m load_speakers_data_from_dir(filename, exclude_speaker_meta)\n\u001B[0;32m    148\u001B[0m     convos_data \u001B[38;5;241m=\u001B[39m load_convos_data_from_dir(filename, exclude_conversation_meta)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raja\\Lib\\site-packages\\convokit\\model\\corpus_helpers.py:120\u001B[0m, in \u001B[0;36mload_utterance_info_from_dir\u001B[1;34m(dirname, utterance_start_index, utterance_end_index, exclude_utterance_meta)\u001B[0m\n\u001B[0;32m    118\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m f:\n\u001B[0;32m    119\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m utterance_start_index \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m idx \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m utterance_end_index:\n\u001B[1;32m--> 120\u001B[0m                 utterances\u001B[38;5;241m.\u001B[39mappend(\u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mloads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mline\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    121\u001B[0m             idx \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dirname, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutterances.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raja\\Lib\\json\\__init__.py:346\u001B[0m, in \u001B[0;36mloads\u001B[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001B[0m\n\u001B[0;32m    341\u001B[0m     s \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mdecode(detect_encoding(s), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msurrogatepass\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    344\u001B[0m         parse_int \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m parse_float \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    345\u001B[0m         parse_constant \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m object_pairs_hook \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[1;32m--> 346\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_decoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    347\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    348\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONDecoder\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raja\\Lib\\json\\decoder.py:337\u001B[0m, in \u001B[0;36mJSONDecoder.decode\u001B[1;34m(self, s, _w)\u001B[0m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, s, _w\u001B[38;5;241m=\u001B[39mWHITESPACE\u001B[38;5;241m.\u001B[39mmatch):\n\u001B[0;32m    333\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001B[39;00m\n\u001B[0;32m    334\u001B[0m \u001B[38;5;124;03m    containing a JSON document).\u001B[39;00m\n\u001B[0;32m    335\u001B[0m \n\u001B[0;32m    336\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 337\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraw_decode\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_w\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mend\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    338\u001B[0m     end \u001B[38;5;241m=\u001B[39m _w(s, end)\u001B[38;5;241m.\u001B[39mend()\n\u001B[0;32m    339\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m end \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(s):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Raja\\Lib\\json\\decoder.py:353\u001B[0m, in \u001B[0;36mJSONDecoder.raw_decode\u001B[1;34m(self, s, idx)\u001B[0m\n\u001B[0;32m    344\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    350\u001B[0m \n\u001B[0;32m    351\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    352\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 353\u001B[0m     obj, end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscan_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    355\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m JSONDecodeError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpecting value\u001B[39m\u001B[38;5;124m\"\u001B[39m, s, err\u001B[38;5;241m.\u001B[39mvalue) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T23:58:35.357818Z",
     "start_time": "2024-08-06T23:58:35.357818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test = load_df_pickle('utterancesPunR.pkl')\n",
    "print(test.shape)\n",
    "print(test.head())\n",
    "test_trimmed = test[test['meta.score'] > 20]\n",
    "print(test_trimmed.shape)"
   ],
   "id": "7b042c999bf51fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample = test_trimmed.sample(n=1)\n",
    "text = sample['text'].values[0]\n",
    "print(sample['meta.parsed'][0])\n",
    "print(text)\n",
    "print(sentiment_analysis(text))"
   ],
   "id": "f198532e8acaf24f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(analyzer.polarity_scores(text))",
   "id": "20eb21f7872b4278",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "utterances_trimmed = utterances[utterances['meta.score'] > 20]\n",
    "print(utterances_trimmed.shape)\n",
    "utterances_trimmed['meta.sentiment'] = utterances_trimmed['text'].apply(lambda x : analyzer.polarity_scores(x))\n",
    "print(utterances_trimmed.head())"
   ],
   "id": "9ef3a4707e4db91b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "save_df_pickle(utterances_trimmed, 'utterancesTrimmedSentiment.pkl')\n",
   "id": "85654d89aab80bea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = utterances_trimmed['meta.score']\n",
    "y = utterances_trimmed['meta.sentiment'].apply(lambda d: d['neg'])\n",
    "\n",
    "# 创建散点图\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(x, y, color='blue', alpha=0.6)\n",
    "plt.title('Scatter Plot of meta.score vs. meta.trimmed[neg]')\n",
    "plt.xlabel('meta.score')\n",
    "plt.ylabel('meta.trimmed[neg]')\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ],
   "id": "9717c3a258b20644",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(utterances_trimmed['meta.sentiment'].apply(lambda d: d['pos']), utterances_trimmed['meta.sentiment'].apply(lambda d: d['neg']), color='blue', alpha=0.6)\n",
    "plt.title('Scatter Plot of pos.score vs. meta.trimmed[neg]')\n",
    "plt.xlabel('pos.score')\n",
    "plt.ylabel('[neg]')\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ],
   "id": "3e8aa4df22390021",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pop_df = load_df_pickle('utterancesTrimmedSentiment.pkl')\n",
    "pop_df['datetime'] = pd.to_datetime(pop_df['timestamp'], unit='s')\n",
    "print(pop_df['datetime'].head())\n"
   ],
   "id": "cd122024e317928b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pop_df\n",
    "# Extracting sentiment components into separate columns\n",
    "df['neg'] = df['meta.sentiment'].apply(lambda x: x['neg'])\n",
    "df['neu'] = df['meta.sentiment'].apply(lambda x: x['neu'])\n",
    "df['pos'] = df['meta.sentiment'].apply(lambda x: x['pos'])\n",
    "df['compound'] = df['meta.sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Creating subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "\n",
    "# Plotting negative sentiment\n",
    "axes[0, 0].scatter(df['meta.score'], df['neg'], c='red')\n",
    "axes[0, 0].set_title('Meta Score vs Negative Sentiment')\n",
    "axes[0, 0].set_xlabel('Meta Score')\n",
    "axes[0, 0].set_ylabel('Negative Sentiment')\n",
    "\n",
    "# Plotting neutral sentiment\n",
    "axes[0, 1].scatter(df['meta.score'], df['neu'], c='blue')\n",
    "axes[0, 1].set_title('Meta Score vs Neutral Sentiment')\n",
    "axes[0, 1].set_xlabel('Meta Score')\n",
    "axes[0, 1].set_ylabel('Neutral Sentiment')\n",
    "\n",
    "# Plotting positive sentiment\n",
    "axes[1, 0].scatter(df['meta.score'], df['pos'], c='green')\n",
    "axes[1, 0].set_title('Meta Score vs Positive Sentiment')\n",
    "axes[1, 0].set_xlabel('Meta Score')\n",
    "axes[1, 0].set_ylabel('Positive Sentiment')\n",
    "\n",
    "# Plotting compound sentiment\n",
    "axes[1, 1].scatter(df['meta.score'], df['compound'], c='purple')\n",
    "axes[1, 1].set_title('Meta Score vs Compound Sentiment')\n",
    "axes[1, 1].set_xlabel('Meta Score')\n",
    "axes[1, 1].set_ylabel('Compound Sentiment')\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "21ac82458b37f840",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extracting sentiment components into separate columns\n",
    "pop_df['neg'] = pop_df['meta.sentiment'].apply(lambda x: x['neg'])\n",
    "pop_df['neu'] = pop_df['meta.sentiment'].apply(lambda x: x['neu'])\n",
    "pop_df['pos'] = pop_df['meta.sentiment'].apply(lambda x: x['pos'])\n",
    "pop_df['compound'] = pop_df['meta.sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Creating a single plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plotting negative sentiment\n",
    "plt.scatter(pop_df['meta.score'], pop_df['neg'], c='red', label='Negative Sentiment')\n",
    "\n",
    "# Plotting neutral sentiment\n",
    "plt.scatter(pop_df['meta.score'], pop_df['neu'], c='blue', label='Neutral Sentiment')\n",
    "\n",
    "# Plotting positive sentiment\n",
    "plt.scatter(pop_df['meta.score'], pop_df['pos'], c='green', label='Positive Sentiment')\n",
    "\n",
    "# Plotting compound sentiment\n",
    "plt.scatter(pop_df['meta.score'], pop_df['compound'], c='purple', label='Compound Sentiment')\n",
    "\n",
    "plt.title('Meta Score vs Sentiment Components')\n",
    "plt.xlabel('Meta Score')\n",
    "plt.ylabel('Sentiment Value')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "b489956564329be2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting sentiment components into separate columns\n",
    "pop_df['neg'] = pop_df['meta.sentiment'].apply(lambda x: x['neg'])\n",
    "pop_df['neu'] = pop_df['meta.sentiment'].apply(lambda x: x['neu'])\n",
    "pop_df['pos'] = pop_df['meta.sentiment'].apply(lambda x: x['pos'])\n",
    "pop_df['compound'] = pop_df['meta.sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Creating histograms\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "\n",
    "# Histogram for negative sentiment\n",
    "axes[0, 0].hist(pop_df['neg'], bins=30, color='red', edgecolor='black')\n",
    "axes[0, 0].set_title('Negative Sentiment Distribution')\n",
    "axes[0, 0].set_xlabel('Negative Sentiment')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Histogram for neutral sentiment\n",
    "axes[0, 1].hist(pop_df['neu'], bins=30, color='blue', edgecolor='black')\n",
    "axes[0, 1].set_title('Neutral Sentiment Distribution')\n",
    "axes[0, 1].set_xlabel('Neutral Sentiment')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Histogram for positive sentiment\n",
    "axes[1, 0].hist(pop_df['pos'], bins=30, color='green', edgecolor='black')\n",
    "axes[1, 0].set_title('Positive Sentiment Distribution')\n",
    "axes[1, 0].set_xlabel('Positive Sentiment')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Histogram for compound sentiment\n",
    "axes[1, 1].hist(pop_df['compound'], bins=30, color='purple', edgecolor='black')\n",
    "axes[1, 1].set_title('Compound Sentiment Distribution')\n",
    "axes[1, 1].set_xlabel('Compound Sentiment')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "1a23a55380fc86a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting sentiment components into separate columns\n",
    "pop_df['neg'] = pop_df['meta.sentiment'].apply(lambda x: x['neg'])\n",
    "pop_df['neu'] = pop_df['meta.sentiment'].apply(lambda x: x['neu'])\n",
    "pop_df['pos'] = pop_df['meta.sentiment'].apply(lambda x: x['pos'])\n",
    "pop_df['compound'] = pop_df['meta.sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Grouping by 'meta.score' and calculating the mean of 'pos', 'neg', 'neu', and 'compound'\n",
    "grouped_df = pop_df.groupby('meta.score').agg({'pos': 'mean', 'neg': 'mean', 'neu': 'mean', 'compound': 'mean'}).reset_index()\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plotting average positive sentiment\n",
    "plt.plot(grouped_df['meta.score'], grouped_df['pos'], marker='o', label='Average Positive Sentiment', color='green')\n",
    "\n",
    "# Plotting average negative sentiment\n",
    "plt.plot(grouped_df['meta.score'], grouped_df['neg'], marker='o', label='Average Negative Sentiment', color='red')\n",
    "\n",
    "# Plotting average neutral sentiment\n",
    "plt.plot(grouped_df['meta.score'], grouped_df['neu'], marker='o', label='Average Neutral Sentiment', color='blue')\n",
    "\n",
    "# Plotting average compound sentiment\n",
    "plt.plot(grouped_df['meta.score'], grouped_df['compound'], marker='o', label='Average Compound Sentiment', color='purple')\n",
    "\n",
    "plt.title('Average Sentiments by Meta Score')\n",
    "plt.xlabel('Meta Score')\n",
    "plt.ylabel('Average Sentiment Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "582931f1135e103a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting sentiment components into separate columns\n",
    "pop_df['neg'] = pop_df['meta.sentiment'].apply(lambda x: x['neg'])\n",
    "pop_df['neu'] = pop_df['meta.sentiment'].apply(lambda x: x['neu'])\n",
    "pop_df['pos'] = pop_df['meta.sentiment'].apply(lambda x: x['pos'])\n",
    "pop_df['compound'] = pop_df['meta.sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "# Creating subplots for box plots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 12))\n",
    "\n",
    "# Box plot for positive sentiment\n",
    "pop_df.boxplot(column='pos', by='meta.score', ax=axes[0, 0], grid=False)\n",
    "axes[0, 0].set_title('Positive Sentiment')\n",
    "axes[0, 0].set_xlabel('Meta Score')\n",
    "axes[0, 0].set_ylabel('Positive Sentiment')\n",
    "axes[0, 0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Box plot for negative sentiment\n",
    "pop_df.boxplot(column='neg', by='meta.score', ax=axes[0, 1], grid=False)\n",
    "axes[0, 1].set_title('Negative Sentiment')\n",
    "axes[0, 1].set_xlabel('Meta Score')\n",
    "axes[0, 1].set_ylabel('Negative Sentiment')\n",
    "axes[0, 1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Box plot for neutral sentiment\n",
    "pop_df.boxplot(column='neu', by='meta.score', ax=axes[1, 0], grid=False)\n",
    "axes[1, 0].set_title('Neutral Sentiment')\n",
    "axes[1, 0].set_xlabel('Meta Score')\n",
    "axes[1, 0].set_ylabel('Neutral Sentiment')\n",
    "axes[1, 0].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Box plot for compound sentiment\n",
    "pop_df.boxplot(column='compound', by='meta.score', ax=axes[1, 1], grid=False)\n",
    "axes[1, 1].set_title('Compound Sentiment')\n",
    "axes[1, 1].set_xlabel('Meta Score')\n",
    "axes[1, 1].set_ylabel('Compound Sentiment')\n",
    "axes[1, 1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.suptitle('')  # Suppress the overall title to avoid overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "deceaa93b2dcbc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corpus = Corpus(filename=path+'conversations-gone-awry-cmv-corpus')\n",
    "speakers, conversations, utterances = load_dfs(corpus)\n",
    "print_overview(speakers, conversations, utterances)\n",
    "check_column_type(utterances)"
   ],
   "id": "c5f00f320408169e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "utterances_trimmed = load_df_pickle('utterancesTrimmedSentiment.pkl')\n",
    "new_corpus = Corpus.from_pandas(utterances_df=utterances_trimmed, speakers_df=speakers, conversations_df=conversations)\n",
    "new_corpus.print_summary_stats()"
   ],
   "id": "4dc573735ed9ba3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4f2aa6bda37d5e86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_utt_df_sentiment(df):\n",
    "    df['meta.sentiment'] = df['text'].apply(lambda x : analyzer.polarity_scores(x))\n",
    "    # Extracting sentiment components into separate columns\n",
    "    df['neg'] = df['meta.sentiment'].apply(lambda x: x['neg'])\n",
    "    df['neu'] = df['meta.sentiment'].apply(lambda x: x['neu'])\n",
    "    df['pos'] = df['meta.sentiment'].apply(lambda x: x['pos'])\n",
    "    df['compound'] = df['meta.sentiment'].apply(lambda x: x['compound'])\n",
    "\n",
    "    # Creating histograms\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "\n",
    "    # Histogram for negative sentiment\n",
    "    axes[0, 0].hist(df['neg'], bins=30, color='red', edgecolor='black')\n",
    "    axes[0, 0].set_title('Negative Sentiment Distribution')\n",
    "    axes[0, 0].set_xlabel('Negative Sentiment')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "    # Histogram for neutral sentiment\n",
    "    axes[0, 1].hist(df['neu'], bins=30, color='blue', edgecolor='black')\n",
    "    axes[0, 1].set_title('Neutral Sentiment Distribution')\n",
    "    axes[0, 1].set_xlabel('Neutral Sentiment')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "    # Histogram for positive sentiment\n",
    "    axes[1, 0].hist(df['pos'], bins=30, color='green', edgecolor='black')\n",
    "    axes[1, 0].set_title('Positive Sentiment Distribution')\n",
    "    axes[1, 0].set_xlabel('Positive Sentiment')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "    # Histogram for compound sentiment\n",
    "    axes[1, 1].hist(df['compound'], bins=30, color='purple', edgecolor='black')\n",
    "    axes[1, 1].set_title('Compound Sentiment Distribution')\n",
    "    axes[1, 1].set_xlabel('Compound Sentiment')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ],
   "id": "3adfb7ca66d81ffb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for conv in corpus.iter_conversations():\n",
    "    # print(len(conv._utterance_ids))\n",
    "    if len(conv.get_utterance_ids()) > 21 and len(conv.get_speaker_ids()) < 10:\n",
    "        df = conv.get_utterances_dataframe()\n",
    "        print(df.shape)\n",
    "        print(df.head())\n",
    "        print(df.columns)\n",
    "        plot_utt_df_sentiment(df)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    print(conv.print_conversation_stats())\n",
    "    print(conv.get_speaker_ids())\n",
    "    break\n",
    "\n",
    "def utter_df_from_corpus_condition_conv(corpus, condiion = None):\n",
    "    for conv in corpus.iter_conversations():\n",
    "        condition = True if (len(conv.get_utterance_ids()>20 and len(conv.get_speaker_ids())<10)) else False\n",
    "        \n",
    "        # print(len(conv._utterance_ids))\n",
    "        if condition:\n",
    "            df = conv.get_utterances_dataframe()\n",
    "            print(df.shape)\n",
    "            print(df.head())\n",
    "            print(df.columns)\n",
    "            plot_utt_df_sentiment(df)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "        print(conv.print_conversation_stats())\n",
    "        print(conv.get_speaker_ids())\n",
    "        break"
   ],
   "id": "55274e2aaabd13f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# df = utterances_trimmed\n",
    "# df['meta.sentiment'] = df['text'].apply(lambda x : analyzer.polarity_scores(x))\n",
    "# # Extracting sentiment components into separate columns\n",
    "# df['neg'] = df['meta.sentiment'].apply(lambda x: x['neg'])\n",
    "# df['neu'] = df['meta.sentiment'].apply(lambda x: x['neu'])\n",
    "# df['pos'] = df['meta.sentiment'].apply(lambda x: x['pos'])\n",
    "# df['compound'] = df['meta.sentiment'].apply(lambda x: x['compound'])\n",
    "# # Step 1: List unique values in the 'speaker' column\n",
    "# unique_speakers = df['speaker'].unique()\n",
    "# \n",
    "# # Step 2: Assign a unique color to each speaker\n",
    "# colors = plt.cm.get_cmap('tab10', len(unique_speakers))\n",
    "# \n",
    "# # Create a dictionary to map speakers to colors\n",
    "# speaker_color_map = {speaker: colors(i) for i, speaker in enumerate(unique_speakers)}\n",
    "# \n",
    "# # Step 3: Plotting\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# \n",
    "# # Iterate over each speaker and plot their data\n",
    "# for speaker in unique_speakers:\n",
    "#     speaker_data = df[df['speaker'] == speaker]\n",
    "#     x = speaker_data['timestamp']\n",
    "#     y = speaker_data['pos'] - speaker_data['neg']\n",
    "#     plt.plot(x, y, label=speaker, color=speaker_color_map[speaker])\n",
    "# \n",
    "# # Adding labels and title\n",
    "# plt.xlabel('Timestamp')\n",
    "# plt.ylabel('pos - neg')\n",
    "# plt.title('pos - neg Over Time by Speaker')\n",
    "# plt.legend(title='Speaker')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ],
   "id": "ec56415d18cc03d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_conv_id_list(corpus, condition):\n",
    "    conv_id_list = []\n",
    "    for conv in corpus.iter_conversations():\n",
    "        if condition(conv):\n",
    "            conv_id_list.append(conv.id)\n",
    "    return conv_id_list\n",
    "\n",
    "def condition(conv):\n",
    "    return (len(conv.get_utterance_ids()) > 15 and len(conv.get_speaker_ids()) < 10)\n",
    "\n",
    "conv_id_list = get_conv_id_list(corpus, condition)\n",
    "print(conv_id_list)"
   ],
   "id": "3923caeaf18bcee4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_conversation_sentiment_trend(conv):\n",
    "    df = conv.get_utterances_dataframe()\n",
    "    df['meta.sentiment'] = df['text'].apply(lambda x : analyzer.polarity_scores(x))\n",
    "    df['neg'] = df['meta.sentiment'].apply(lambda x: x['neg'])\n",
    "    df['neu'] = df['meta.sentiment'].apply(lambda x: x['neu'])\n",
    "    df['pos'] = df['meta.sentiment'].apply(lambda x: x['pos'])\n",
    "    df['compound'] = df['meta.sentiment'].apply(lambda x: x['compound'])\n",
    "    \n",
    "    # Plotting the sentiment trend\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(df['timestamp'], df['neg'], label='Negative Sentiment', color='red', linewidth=2)\n",
    "    plt.plot(df['timestamp'], df['neu'], label='Neutral Sentiment', color='blue', linewidth=2)\n",
    "    plt.plot(df['timestamp'], df['pos'], label='Positive Sentiment', color='green', linewidth=2)\n",
    "    plt.plot(df['timestamp'], df['compound'], label='Compound Sentiment', color='purple', linewidth=2)\n",
    "    plt.title('Sentiment Trend in Conversation')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('Sentiment Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for conv in corpus.iter_conversations():\n",
    "    if conv.id in conv_id_list:\n",
    "        df = conv.get_utterances_dataframe()\n",
    "        print(df.shape)\n",
    "        # print(df.head())\n",
    "        print(df.columns)\n",
    "        plot_conversation_sentiment_trend(conv)"
   ],
   "id": "27dee1a96f9a56fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print text for conversations. id:['cwh7b8s', 'd27fgau', 'd61kp2c']\n",
    "import os, pandas as pd\n",
    "from convokit import Corpus, Speaker, Utterance, Conversation, download\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "path = \"C:/Users/L/.convokit/downloads/\"\n",
    "corpus = Corpus(filename=path+'conversations-gone-awry-cmv-corpus')\n",
    "\n",
    "for conv in corpus.iter_conversations():\n",
    "    if conv.id in ['cwh7b8s', 'd27fgau', 'd61kp2c']:\n",
    "        print(conv.meta)\n",
    "        df = conv.get_utterances_dataframe()\n",
    "        print(\"$$$$$$$$$$$$$$$\\n\\n\\n\\n\")\n",
    "        for i,text in enumerate(df['text'].values):\n",
    "            print(i,\"\\n\")\n",
    "            print(analyzer.polarity_scores(text))\n",
    "            print(text)"
   ],
   "id": "29057372cc9faab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 检查第一个utterance是否是comment\n",
    "\n",
    "# from convokit import Corpus, download\n",
    "# \n",
    "# # 下载示例语料库\n",
    "# corpus = Corpus(filename= path + 'subreddit-ADD')\n",
    "# # 遍历对话\n",
    "# for conv in corpus.iter_conversations():\n",
    "#     # 获取对话中的所有utterance\n",
    "#     utterances = list(conv.iter_utterances())\n",
    "\n",
    "    # 检查第一个utterance\n",
    "    # first_utterance = utterances[0]\n",
    "    # if first_utterance.meta.get('is_post', False):\n",
    "    #     print(f\"Conversation {conv.id} starts with a post.\")\n",
    "    # else:\n",
    "    #     print(f\"Conversation {conv.id} starts with a comment.\")"
   ],
   "id": "5426acca0f1f86a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "def generate_answer_with_ollama(query, context=' ', model=\"llama3:latest\", url=\"http://localhost:11434/api/generate\"):\n",
    "    input_text = f\"Question: {query}\\nContext: {context}\"\n",
    "    payload = {\n",
    "        'prompt': input_text,\n",
    "        'model': model,  # Add the model parameter here\n",
    "        'max_tokens': 50  # Set the maximum number of tokens to generate\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "\n",
    "    try:\n",
    "        # Process NDJSON response\n",
    "        responses = response.content.decode('utf-8').splitlines()\n",
    "        json_responses = [requests.models.complexjson.loads(line) for line in responses]\n",
    "        responses = [item[\"response\"] for item in json_responses]\n",
    "        text = ''.join(responses)\n",
    "        # print(\"NDJSON parsed successfully:\", text)\n",
    "\n",
    "        # Extract the answer from the first JSON object\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(\"Failed to decode NDJSON response:\", e)\n",
    "        print(\"Response content:\", response.content)\n",
    "        raise\n",
    "\n",
    "    print(\"Response content:\", response.content)\n",
    "    raise Exception(f\"Unexpected content type: {content_type}\")\n",
    "    \n",
    "    \n",
    "generate_answer_with_ollama('Seperate the following text into meaning--complete sentences:\\n\\n', context=\"\"\"Why then? Call a fucking pharmacist who can tell you what the consequences are, IF ANY. That's their job. Or pay the money to talk to a doctor on a holiday. That is what you do. Don't come on here and post the question to a bunch of people who are unqualified to answer. Get off the fucking computer and call someone. Jesus Christ.\"\"\")"
   ],
   "id": "bfb3e121231a2684",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def rename_speaker_conv(conv):\n",
    "    # get all possible speaker id in the dataframe, then map each one to A,B,C,D..... in a new column\n",
    "    nameMap = {}\n",
    "    for speaker in conv.get_speaker_ids():\n",
    "        nameMap[speaker] = chr(65+len(nameMap))\n",
    "        \n",
    "    utts_df = conv.get_utterances_dataframe()\n",
    "    utts_df['speakerID'] = utts_df['speaker'].apply(lambda x: nameMap[x])\n",
    "\n",
    "    return utts_df\n",
    "\n",
    "def format_conversation_user_utt_df(utt_df):\n",
    "    \n",
    "    speaker = utt_df['speakerID'].values\n",
    "    text = utt_df['text'].values\n",
    "    return [f\"{speaker[i]}: {text[i]}\\r\" for i in range(len(speaker))]\n",
    "\n",
    "for conv in corpus.iter_conversations():\n",
    "    if conv.id in ['cwh7b8s', 'd27fgau', 'd61kp2c']:\n",
    "        print(1)\n",
    "        # print(conv.meta)\n",
    "        df = conv.get_utterances_dataframe()\n",
    "        df = rename_speaker_conv(conv)\n",
    "        print(df['speakerID'].values)\n",
    "        print(df['reply_to'].values)\n",
    "        print(df['speaker'].values)\n",
    "        print(df.index)\n",
    "        print(\"$$$$$$$$$$$$$$$\\n\\n\\n\\n\")\n",
    "        for i,text in enumerate(df['text'].values):\n",
    "            print(i,\"\\n\")\n",
    "            print(analyzer.polarity_scores(text))\n",
    "            # print(text)\n",
    "            \n"
   ],
   "id": "3c8dfddb2400f662",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 示例文本列表\n",
    "texts = [\n",
    "    \"I love natural language processing. It's fascinating and fun!\",\n",
    "    \"Machine learning is a key component of artificial intelligence.\",\n",
    "    \"Deep learning models are very powerful for image recognition.\",\n",
    "    \"Natural language processing involves understanding and generating text.\",\n",
    "    \"Artificial intelligence is transforming many industries.\"\n",
    "]\n",
    "\n",
    "utt_df = rename_speaker_conv(corpus.get_conversation('cwh7b8s'))\n",
    "\n",
    "texts  = format_conversation_user_utt_df(utt_df)\n",
    "# ��始化TF-IDF向量化器\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 计算TF-IDF矩阵\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# 获取特征名称（关键词）\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# 将TF-IDF矩阵转换为DataFrame\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "\n",
    "# 打印每个文本的关键词及其权重\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Text {i+1}: {text}\")\n",
    "    print(\"Keywords and their weights:\")\n",
    "    for keyword, weight in zip(df_tfidf.columns, df_tfidf.iloc[i]):\n",
    "        if weight > 0:\n",
    "            print(f\"{keyword}: {weight:.4f}\")\n",
    "    print(\"\\n\")"
   ],
   "id": "734f5e515938ace4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3a6b1dcf2a8307af",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

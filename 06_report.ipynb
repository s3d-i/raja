{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cfe2eb",
   "metadata": {},
   "source": [
    "# 06 - Report & Summary\n",
    "Roll up refusal, toxicity, and topic signals into a compact report artifact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8254ad",
   "metadata": {},
   "source": [
    "**Goals**\n",
    "- Load processed imitation + Perspective bundles (including the more_refuse variant).\n",
    "- Derive refusal using true_rate when available and summarize refusal/toxicity metrics.\n",
    "- Surface topic highlights from migrated LDA assets and persist a lightweight report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca999ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from utils.data_io import load_df_list_pickle, flatten_conversation_bundles, describe_bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3cab7d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/l/Raja_win/cleaned/assets/processed')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paths and toggles\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "ASSETS_RAW = PROJECT_ROOT / \"assets\" / \"raw\"\n",
    "ASSETS_PROCESSED = PROJECT_ROOT / \"assets\" / \"processed\"\n",
    "ASSETS_TOPICS = ASSETS_PROCESSED / \"topics\"\n",
    "REPORT_DIR = PROJECT_ROOT / \"report\"\n",
    "\n",
    "IMMITATION_PATH = ASSETS_PROCESSED / \"combat_threads_with_imitation.pkl\"\n",
    "PERSPECTIVE_PATH = ASSETS_PROCESSED / \"combat_threads_with_perspective.pkl\"\n",
    "PERSPECTIVE_LIST_PATH = ASSETS_PROCESSED / \"combat_threads_with_perspective_list.pkl\"\n",
    "PERSPECTIVE_MORE_REFUSE_PATH = ASSETS_PROCESSED / \"combat_threads_with_perspective_list_more_refuse_cleaned.pkl\"\n",
    "\n",
    "LDA_TFIDF_REF = ASSETS_TOPICS / \"lda_results_tfidf_ref.pkl\"\n",
    "LDA_TFIDF_ACC = ASSETS_TOPICS / \"lda_results_tfidf_acc.pkl\"\n",
    "LDA_COUNT_REF = ASSETS_TOPICS / \"lda_results_count_ref.pkl\"\n",
    "LDA_COUNT_ACC = ASSETS_TOPICS / \"lda_results_count_acc.pkl\"\n",
    "\n",
    "SOURCE_MODE = \"more_refuse\"  # options: more_refuse, perspective_list, base\n",
    "HIGH_TOXICITY = 0.5  # label a turn as toxic when Perspective TOXICITY >= this\n",
    "REFUSAL_CONVERSATION_THRESHOLD = 0.1  # bucket conversations by refusal share\n",
    "REFUSAL_TRUE_RATE_THRESHOLD = 0.67  # more_refuse bundle marks refusal when true_rate falls below this share\n",
    "\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ASSETS_PROCESSED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea710c",
   "metadata": {},
   "source": [
    "### Asset manifest\n",
    "List primary inputs and migrated topic assets used to generate the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdc4812d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>role</th>\n",
       "      <th>path</th>\n",
       "      <th>note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>input</td>\n",
       "      <td>/home/l/Raja_win/cleaned/assets/processed/comb...</td>\n",
       "      <td>Imitation bundle with imm_1 + imm_1_check (sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>input</td>\n",
       "      <td>/home/l/Raja_win/cleaned/assets/processed/comb...</td>\n",
       "      <td>Imitation + raw Perspective dicts (source: Raj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>variant_optional</td>\n",
       "      <td>/home/l/Raja_win/cleaned/assets/processed/comb...</td>\n",
       "      <td>Perspective list-encoded scores (source: Raja/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>variant_optional</td>\n",
       "      <td>/home/l/Raja_win/cleaned/assets/processed/comb...</td>\n",
       "      <td>Perspective list + refuse_add/true_rate heuris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>topics_optional</td>\n",
       "      <td>/home/l/Raja_win/cleaned/assets/processed/topi...</td>\n",
       "      <td>TF-IDF LDA on refusal conversations (migrated ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>topics_optional</td>\n",
       "      <td>/home/l/Raja_win/cleaned/assets/processed/topi...</td>\n",
       "      <td>TF-IDF LDA on acceptance conversations (migrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>topics_optional</td>\n",
       "      <td>/home/l/Raja_win/cleaned/assets/processed/topi...</td>\n",
       "      <td>Count-vector LDA on refusal conversations (mig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>topics_optional</td>\n",
       "      <td>/home/l/Raja_win/cleaned/assets/processed/topi...</td>\n",
       "      <td>Count-vector LDA on acceptance conversations (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>output_optional</td>\n",
       "      <td>/home/l/Raja_win/cleaned/report/report_more_re...</td>\n",
       "      <td>Summary JSON written by this notebook (metrics...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               role                                               path  \\\n",
       "0             input  /home/l/Raja_win/cleaned/assets/processed/comb...   \n",
       "1             input  /home/l/Raja_win/cleaned/assets/processed/comb...   \n",
       "2  variant_optional  /home/l/Raja_win/cleaned/assets/processed/comb...   \n",
       "3  variant_optional  /home/l/Raja_win/cleaned/assets/processed/comb...   \n",
       "4   topics_optional  /home/l/Raja_win/cleaned/assets/processed/topi...   \n",
       "5   topics_optional  /home/l/Raja_win/cleaned/assets/processed/topi...   \n",
       "6   topics_optional  /home/l/Raja_win/cleaned/assets/processed/topi...   \n",
       "7   topics_optional  /home/l/Raja_win/cleaned/assets/processed/topi...   \n",
       "8   output_optional  /home/l/Raja_win/cleaned/report/report_more_re...   \n",
       "\n",
       "                                                note  \n",
       "0  Imitation bundle with imm_1 + imm_1_check (sou...  \n",
       "1  Imitation + raw Perspective dicts (source: Raj...  \n",
       "2  Perspective list-encoded scores (source: Raja/...  \n",
       "3  Perspective list + refuse_add/true_rate heuris...  \n",
       "4  TF-IDF LDA on refusal conversations (migrated ...  \n",
       "5  TF-IDF LDA on acceptance conversations (migrat...  \n",
       "6  Count-vector LDA on refusal conversations (mig...  \n",
       "7  Count-vector LDA on acceptance conversations (...  \n",
       "8  Summary JSON written by this notebook (metrics...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manifest = [\n",
    "    {\n",
    "        'role': 'input',\n",
    "        'path': IMMITATION_PATH,\n",
    "        'note': 'Imitation bundle with imm_1 + imm_1_check (source: Raja/Convo/combat_df_list_imms_1_full.pkl).',\n",
    "    },\n",
    "    {\n",
    "        'role': 'input',\n",
    "        'path': PERSPECTIVE_PATH,\n",
    "        'note': 'Imitation + raw Perspective dicts (source: Raja/revised_convo/combat_df_list_imms_1_full_perspective.pkl).',\n",
    "    },\n",
    "    {\n",
    "        'role': 'variant_optional',\n",
    "        'path': PERSPECTIVE_LIST_PATH,\n",
    "        'note': 'Perspective list-encoded scores (source: Raja/revised_convo/combat_df_list_imms_1_full_perspective_list.pkl).',\n",
    "    },\n",
    "    {\n",
    "        'role': 'variant_optional',\n",
    "        'path': PERSPECTIVE_MORE_REFUSE_PATH,\n",
    "        'note': 'Perspective list + refuse_add/true_rate heuristics (source: Raja/revised_convo/combat_df_list_imms_1_full_perspective_list_moreRefuse_refuseCleaned.pkl).',\n",
    "    },\n",
    "    {\n",
    "        'role': 'topics_optional',\n",
    "        'path': LDA_TFIDF_REF,\n",
    "        'note': 'TF-IDF LDA on refusal conversations (migrated from Raja/revised_convo/lda_results_tfidf_ref.pkl).',\n",
    "    },\n",
    "    {\n",
    "        'role': 'topics_optional',\n",
    "        'path': LDA_TFIDF_ACC,\n",
    "        'note': 'TF-IDF LDA on acceptance conversations (migrated from Raja/revised_convo/lda_results_tfidf_acc.pkl).',\n",
    "    },\n",
    "    {\n",
    "        'role': 'topics_optional',\n",
    "        'path': LDA_COUNT_REF,\n",
    "        'note': 'Count-vector LDA on refusal conversations (migrated from Raja/revised_convo/lda_results_count_ref.pkl).',\n",
    "    },\n",
    "    {\n",
    "        'role': 'topics_optional',\n",
    "        'path': LDA_COUNT_ACC,\n",
    "        'note': 'Count-vector LDA on acceptance conversations (migrated from Raja/revised_convo/lda_results_count_acc.pkl).',\n",
    "    },\n",
    "    {\n",
    "        'role': 'output_optional',\n",
    "        'path': REPORT_DIR / f\"report_{SOURCE_MODE}.json\",\n",
    "        'note': 'Summary JSON written by this notebook (metrics + topic highlights).',\n",
    "    },\n",
    "]\n",
    "\n",
    "pd.DataFrame(manifest)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9c942",
   "metadata": {},
   "source": [
    "### Load bundle and derive refusal\n",
    "Use true_rate when available (more_refuse) otherwise fallback to imm_1_check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d81909e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using source: combat_threads_with_perspective_list_more_refuse_cleaned.pkl\n",
      "bundle summary: {'bundle_len': 6842, 'frame_count': 7139, 'columns': ['imm_1', 'imm_1_check', 'perspective', 'perspective_ls', 'perspective_type', 'refuse_add', 'text', 'true_rate'], 'example_ids': ['cuedywn', 'czb942p', 'czbbocu']}\n",
      "rows 35116\n",
      "columns ['conversation_idx', 'text', 'imm_1', 'imm_1_check', 'perspective', 'perspective_type', 'perspective_ls', 'refuse_add', 'true_rate', 'is_refusal']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_idx</th>\n",
       "      <th>text</th>\n",
       "      <th>imm_1</th>\n",
       "      <th>imm_1_check</th>\n",
       "      <th>perspective</th>\n",
       "      <th>perspective_type</th>\n",
       "      <th>perspective_ls</th>\n",
       "      <th>refuse_add</th>\n",
       "      <th>true_rate</th>\n",
       "      <th>is_refusal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cuedywn</th>\n",
       "      <td>0</td>\n",
       "      <td>Twin studies studies suggest that about 80 per...</td>\n",
       "      <td>\":\\n\"Research into twin studies indicates that...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'PROFANITY': {'spanScores': [{'begin': 0, 'en...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>[80, 0.102570616, 0.08678792, 0.081613354, 0.0...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>czb942p</th>\n",
       "      <td>0</td>\n",
       "      <td>Sorry Perpetualjoke, your comment has been rem...</td>\n",
       "      <td>\"I apologize for the mistake, Perpetualjoke. I...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'CURIOSITY_EXPERIMENTAL': {'spanScores': [{'b...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>[739, 0.24385335, 0.5053742, 0.5939831, 0.0045...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>czbbocu</th>\n",
       "      <td>0</td>\n",
       "      <td>Are you kidding me?You decided to remove it af...</td>\n",
       "      <td>\"Five months wasn't even enough time to get ac...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'PROFANITY_EXPERIMENTAL': {'spanScores': [{'b...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>[59, 0.21123351, 0.18747956, 0.7543965, 0.0034...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>czbdh6q</th>\n",
       "      <td>0</td>\n",
       "      <td>You decided to edit it after 5 months.  Why wo...</td>\n",
       "      <td>\"The silence was deafening, so I took the init...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'SEXUALLY_EXPLICIT': {'spanScores': [{'begin'...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>[79, 0.15358509, 0.096258074, 0.85268605, 0.00...</td>\n",
       "      <td>[True, True, True, False, True]</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>czbe470</th>\n",
       "      <td>0</td>\n",
       "      <td>I just can't believe you are browsing 5 month ...</td>\n",
       "      <td>\"I'm taken aback that you'd assume the worst a...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'SEVERE_TOXICITY_EXPERIMENTAL': {'spanScores'...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>[130, 0.81405926, 0.721688, 0.3337865, 0.00521...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         conversation_idx                                               text  \\\n",
       "id                                                                             \n",
       "cuedywn                 0  Twin studies studies suggest that about 80 per...   \n",
       "czb942p                 0  Sorry Perpetualjoke, your comment has been rem...   \n",
       "czbbocu                 0  Are you kidding me?You decided to remove it af...   \n",
       "czbdh6q                 0  You decided to edit it after 5 months.  Why wo...   \n",
       "czbe470                 0  I just can't believe you are browsing 5 month ...   \n",
       "\n",
       "                                                     imm_1  imm_1_check  \\\n",
       "id                                                                        \n",
       "cuedywn  \":\\n\"Research into twin studies indicates that...         True   \n",
       "czb942p  \"I apologize for the mistake, Perpetualjoke. I...         True   \n",
       "czbbocu  \"Five months wasn't even enough time to get ac...         True   \n",
       "czbdh6q  \"The silence was deafening, so I took the init...         True   \n",
       "czbe470  \"I'm taken aback that you'd assume the worst a...         True   \n",
       "\n",
       "                                               perspective perspective_type  \\\n",
       "id                                                                            \n",
       "cuedywn  {'PROFANITY': {'spanScores': [{'begin': 0, 'en...   <class 'dict'>   \n",
       "czb942p  {'CURIOSITY_EXPERIMENTAL': {'spanScores': [{'b...   <class 'dict'>   \n",
       "czbbocu  {'PROFANITY_EXPERIMENTAL': {'spanScores': [{'b...   <class 'dict'>   \n",
       "czbdh6q  {'SEXUALLY_EXPLICIT': {'spanScores': [{'begin'...   <class 'dict'>   \n",
       "czbe470  {'SEVERE_TOXICITY_EXPERIMENTAL': {'spanScores'...   <class 'dict'>   \n",
       "\n",
       "                                            perspective_ls  \\\n",
       "id                                                           \n",
       "cuedywn  [80, 0.102570616, 0.08678792, 0.081613354, 0.0...   \n",
       "czb942p  [739, 0.24385335, 0.5053742, 0.5939831, 0.0045...   \n",
       "czbbocu  [59, 0.21123351, 0.18747956, 0.7543965, 0.0034...   \n",
       "czbdh6q  [79, 0.15358509, 0.096258074, 0.85268605, 0.00...   \n",
       "czbe470  [130, 0.81405926, 0.721688, 0.3337865, 0.00521...   \n",
       "\n",
       "                              refuse_add  true_rate  is_refusal  \n",
       "id                                                               \n",
       "cuedywn   [True, True, True, True, True]   1.000000       False  \n",
       "czb942p   [True, True, True, True, True]   1.000000       False  \n",
       "czbbocu   [True, True, True, True, True]   1.000000       False  \n",
       "czbdh6q  [True, True, True, False, True]   0.833333       False  \n",
       "czbe470   [True, True, True, True, True]   1.000000       False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def resolve_source(mode: str):\n",
    "    if mode == 'more_refuse' and PERSPECTIVE_MORE_REFUSE_PATH.exists():\n",
    "        return PERSPECTIVE_MORE_REFUSE_PATH\n",
    "    if mode == 'perspective_list' and PERSPECTIVE_LIST_PATH.exists():\n",
    "        return PERSPECTIVE_LIST_PATH\n",
    "    return PERSPECTIVE_PATH\n",
    "\n",
    "def derive_is_refusal(frame: pd.DataFrame, mode: str) -> pd.Series:\n",
    "    if mode == 'more_refuse' and 'true_rate' in frame.columns:\n",
    "        return frame['true_rate'].fillna(0) < REFUSAL_TRUE_RATE_THRESHOLD\n",
    "    return ~frame['imm_1_check'].astype(bool)\n",
    "\n",
    "SOURCE_PATH = resolve_source(SOURCE_MODE)\n",
    "if not SOURCE_PATH.exists():\n",
    "    raise FileNotFoundError(f'Bundle not found: {SOURCE_PATH}')\n",
    "\n",
    "bundle = load_df_list_pickle(SOURCE_PATH)\n",
    "print(f'using source: {SOURCE_PATH.name}')\n",
    "print('bundle summary:', describe_bundle(bundle))\n",
    "\n",
    "flat = flatten_conversation_bundles(bundle)\n",
    "flat['is_refusal'] = derive_is_refusal(flat, SOURCE_MODE)\n",
    "print('rows', len(flat))\n",
    "print('columns', flat.columns.tolist())\n",
    "flat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdff7da",
   "metadata": {},
   "source": [
    "### Expand Perspective scores\n",
    "Normalize Perspective attributes (dict or list) into flat columns for downstream metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92dc6652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_idx</th>\n",
       "      <th>text</th>\n",
       "      <th>imm_1</th>\n",
       "      <th>imm_1_check</th>\n",
       "      <th>perspective</th>\n",
       "      <th>perspective_type</th>\n",
       "      <th>perspective_ls</th>\n",
       "      <th>refuse_add</th>\n",
       "      <th>true_rate</th>\n",
       "      <th>is_refusal</th>\n",
       "      <th>...</th>\n",
       "      <th>persp_respect_experimental</th>\n",
       "      <th>persp_severe_toxicity</th>\n",
       "      <th>persp_severe_toxicity_experimental</th>\n",
       "      <th>persp_sexually_explicit</th>\n",
       "      <th>persp_threat</th>\n",
       "      <th>persp_threat_experimental</th>\n",
       "      <th>persp_toxicity</th>\n",
       "      <th>persp_toxicity_experimental</th>\n",
       "      <th>persp_span_end</th>\n",
       "      <th>is_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Twin studies studies suggest that about 80 per...</td>\n",
       "      <td>\":\\n\"Research into twin studies indicates that...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'PROFANITY': {'spanScores': [{'begin': 0, 'en...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>[80, 0.102570616, 0.08678792, 0.081613354, 0.0...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493807</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.001965</td>\n",
       "      <td>0.021114</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.006596</td>\n",
       "      <td>0.063027</td>\n",
       "      <td>0.063027</td>\n",
       "      <td>80.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Sorry Perpetualjoke, your comment has been rem...</td>\n",
       "      <td>\"I apologize for the mistake, Perpetualjoke. I...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'CURIOSITY_EXPERIMENTAL': {'spanScores': [{'b...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>[739, 0.24385335, 0.5053742, 0.5939831, 0.0045...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559326</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.010026</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>0.007288</td>\n",
       "      <td>0.035927</td>\n",
       "      <td>0.035927</td>\n",
       "      <td>739.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Are you kidding me?You decided to remove it af...</td>\n",
       "      <td>\"Five months wasn't even enough time to get ac...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'PROFANITY_EXPERIMENTAL': {'spanScores': [{'b...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>[59, 0.21123351, 0.18747956, 0.7543965, 0.0034...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199241</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>0.010675</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>0.009541</td>\n",
       "      <td>0.101325</td>\n",
       "      <td>0.101325</td>\n",
       "      <td>59.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>You decided to edit it after 5 months.  Why wo...</td>\n",
       "      <td>\"The silence was deafening, so I took the init...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'SEXUALLY_EXPLICIT': {'spanScores': [{'begin'...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>[79, 0.15358509, 0.096258074, 0.85268605, 0.00...</td>\n",
       "      <td>[True, True, True, False, True]</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221261</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>0.007495</td>\n",
       "      <td>0.007495</td>\n",
       "      <td>0.031685</td>\n",
       "      <td>0.031685</td>\n",
       "      <td>79.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>I just can't believe you are browsing 5 month ...</td>\n",
       "      <td>\"I'm taken aback that you'd assume the worst a...</td>\n",
       "      <td>True</td>\n",
       "      <td>{'SEVERE_TOXICITY_EXPERIMENTAL': {'spanScores'...</td>\n",
       "      <td>&lt;class 'dict'&gt;</td>\n",
       "      <td>[130, 0.81405926, 0.721688, 0.3337865, 0.00521...</td>\n",
       "      <td>[True, True, True, True, True]</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.433278</td>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.012149</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>0.238121</td>\n",
       "      <td>0.238121</td>\n",
       "      <td>130.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_idx                                               text  \\\n",
       "0                 0  Twin studies studies suggest that about 80 per...   \n",
       "1                 0  Sorry Perpetualjoke, your comment has been rem...   \n",
       "2                 0  Are you kidding me?You decided to remove it af...   \n",
       "3                 0  You decided to edit it after 5 months.  Why wo...   \n",
       "4                 0  I just can't believe you are browsing 5 month ...   \n",
       "\n",
       "                                               imm_1  imm_1_check  \\\n",
       "0  \":\\n\"Research into twin studies indicates that...         True   \n",
       "1  \"I apologize for the mistake, Perpetualjoke. I...         True   \n",
       "2  \"Five months wasn't even enough time to get ac...         True   \n",
       "3  \"The silence was deafening, so I took the init...         True   \n",
       "4  \"I'm taken aback that you'd assume the worst a...         True   \n",
       "\n",
       "                                         perspective perspective_type  \\\n",
       "0  {'PROFANITY': {'spanScores': [{'begin': 0, 'en...   <class 'dict'>   \n",
       "1  {'CURIOSITY_EXPERIMENTAL': {'spanScores': [{'b...   <class 'dict'>   \n",
       "2  {'PROFANITY_EXPERIMENTAL': {'spanScores': [{'b...   <class 'dict'>   \n",
       "3  {'SEXUALLY_EXPLICIT': {'spanScores': [{'begin'...   <class 'dict'>   \n",
       "4  {'SEVERE_TOXICITY_EXPERIMENTAL': {'spanScores'...   <class 'dict'>   \n",
       "\n",
       "                                      perspective_ls  \\\n",
       "0  [80, 0.102570616, 0.08678792, 0.081613354, 0.0...   \n",
       "1  [739, 0.24385335, 0.5053742, 0.5939831, 0.0045...   \n",
       "2  [59, 0.21123351, 0.18747956, 0.7543965, 0.0034...   \n",
       "3  [79, 0.15358509, 0.096258074, 0.85268605, 0.00...   \n",
       "4  [130, 0.81405926, 0.721688, 0.3337865, 0.00521...   \n",
       "\n",
       "                        refuse_add  true_rate  is_refusal  ...  \\\n",
       "0   [True, True, True, True, True]   1.000000       False  ...   \n",
       "1   [True, True, True, True, True]   1.000000       False  ...   \n",
       "2   [True, True, True, True, True]   1.000000       False  ...   \n",
       "3  [True, True, True, False, True]   0.833333       False  ...   \n",
       "4   [True, True, True, True, True]   1.000000       False  ...   \n",
       "\n",
       "   persp_respect_experimental  persp_severe_toxicity  \\\n",
       "0                    0.493807               0.001965   \n",
       "1                    0.559326               0.001640   \n",
       "2                    0.199241               0.002337   \n",
       "3                    0.221261               0.000868   \n",
       "4                    0.433278               0.003624   \n",
       "\n",
       "   persp_severe_toxicity_experimental  persp_sexually_explicit  persp_threat  \\\n",
       "0                            0.001965                 0.021114      0.006596   \n",
       "1                            0.001640                 0.010026      0.007288   \n",
       "2                            0.002337                 0.010675      0.009541   \n",
       "3                            0.000868                 0.004305      0.007495   \n",
       "4                            0.003624                 0.012149      0.009593   \n",
       "\n",
       "   persp_threat_experimental  persp_toxicity  persp_toxicity_experimental  \\\n",
       "0                   0.006596        0.063027                     0.063027   \n",
       "1                   0.007288        0.035927                     0.035927   \n",
       "2                   0.009541        0.101325                     0.101325   \n",
       "3                   0.007495        0.031685                     0.031685   \n",
       "4                   0.009593        0.238121                     0.238121   \n",
       "\n",
       "   persp_span_end  is_toxic  \n",
       "0            80.0     False  \n",
       "1           739.0     False  \n",
       "2            59.0     False  \n",
       "3            79.0     False  \n",
       "4           130.0     False  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERSPECTIVE_ATTRIBUTES = [\n",
    "    'AFFINITY_EXPERIMENTAL',\n",
    "    'COMPASSION_EXPERIMENTAL',\n",
    "    'CURIOSITY_EXPERIMENTAL',\n",
    "    'IDENTITY_ATTACK',\n",
    "    'IDENTITY_ATTACK_EXPERIMENTAL',\n",
    "    'INSULT',\n",
    "    'INSULT_EXPERIMENTAL',\n",
    "    'NUANCE_EXPERIMENTAL',\n",
    "    'PERSONAL_STORY_EXPERIMENTAL',\n",
    "    'PROFANITY',\n",
    "    'PROFANITY_EXPERIMENTAL',\n",
    "    'REASONING_EXPERIMENTAL',\n",
    "    'RESPECT_EXPERIMENTAL',\n",
    "    'SEVERE_TOXICITY',\n",
    "    'SEVERE_TOXICITY_EXPERIMENTAL',\n",
    "    'SEXUALLY_EXPLICIT',\n",
    "    'THREAT',\n",
    "    'THREAT_EXPERIMENTAL',\n",
    "    'TOXICITY',\n",
    "    'TOXICITY_EXPERIMENTAL',\n",
    "]\n",
    "\n",
    "def extract_summary_scores(entry: Optional[dict], attributes: Sequence[str] = PERSPECTIVE_ATTRIBUTES) -> Dict[str, Optional[float]]:\n",
    "    scores: Dict[str, Optional[float]] = {}\n",
    "    for attr in attributes:\n",
    "        key = f'persp_{attr.lower()}'\n",
    "        value = None\n",
    "        if isinstance(entry, dict):\n",
    "            value = entry.get(attr, {}).get('summaryScore', {}).get('value')\n",
    "        scores[key] = value\n",
    "    span_end = None\n",
    "    if isinstance(entry, dict):\n",
    "        spans = entry.get(attributes[0], {}).get('spanScores', [])\n",
    "        if spans:\n",
    "            span_end = spans[0].get('end')\n",
    "    scores['persp_span_end'] = span_end\n",
    "    return scores\n",
    "\n",
    "def perspective_row_to_dict(row: pd.Series) -> Dict[str, Optional[float]]:\n",
    "    ls = row.get('perspective_ls')\n",
    "    if isinstance(ls, (list, tuple)) and len(ls) >= len(PERSPECTIVE_ATTRIBUTES) + 1:\n",
    "        scores = {f'persp_{attr.lower()}': val for attr, val in zip(PERSPECTIVE_ATTRIBUTES, ls[1:])}\n",
    "        scores['persp_span_end'] = ls[0]\n",
    "        return scores\n",
    "    return extract_summary_scores(row.get('perspective'))\n",
    "\n",
    "flat_reset = flat.reset_index(drop=True)\n",
    "score_frame = pd.DataFrame(flat_reset.apply(perspective_row_to_dict, axis=1).tolist())\n",
    "analysis_df = pd.concat([flat_reset, score_frame], axis=1)\n",
    "analysis_df['is_toxic'] = analysis_df['persp_toxicity'].fillna(0) >= HIGH_TOXICITY\n",
    "analysis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e40b71f",
   "metadata": {},
   "source": [
    "### Summary metrics\n",
    "Refusal shares and toxicity breakdowns (overall vs refusal vs acceptance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02d27eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rows                          35116.000000\n",
       "refusal_rate                      0.089133\n",
       "refusal_conversation_share        0.279737\n",
       "toxicity_rate                     0.056584\n",
       "toxicity_rate_refusals            0.226837\n",
       "toxicity_rate_acceptances         0.039924\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refusal_rate = analysis_df['is_refusal'].mean()\n",
    "convo_refusal = analysis_df.groupby('conversation_idx')['is_refusal'].mean()\n",
    "refusal_convo_share = (convo_refusal > REFUSAL_CONVERSATION_THRESHOLD).mean()\n",
    "\n",
    "toxicity_rate = analysis_df['is_toxic'].mean()\n",
    "toxicity_refuse = analysis_df.loc[analysis_df['is_refusal'], 'is_toxic'].mean()\n",
    "toxicity_accept = analysis_df.loc[~analysis_df['is_refusal'], 'is_toxic'].mean()\n",
    "\n",
    "summary_metrics = {\n",
    "    'rows': len(analysis_df),\n",
    "    'refusal_rate': refusal_rate,\n",
    "    'refusal_conversation_share': refusal_convo_share,\n",
    "    'toxicity_rate': toxicity_rate,\n",
    "    'toxicity_rate_refusals': toxicity_refuse,\n",
    "    'toxicity_rate_acceptances': toxicity_accept,\n",
    "}\n",
    "\n",
    "pd.Series(summary_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7f45ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_idx</th>\n",
       "      <th>text</th>\n",
       "      <th>imm_1</th>\n",
       "      <th>is_refusal</th>\n",
       "      <th>persp_toxicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4852</th>\n",
       "      <td>928</td>\n",
       "      <td>the average human (including myself) is an ign...</td>\n",
       "      <td>the average human (including myself) is an ign...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.950486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7747</th>\n",
       "      <td>1468</td>\n",
       "      <td>Id think my parents were fucking morons.</td>\n",
       "      <td>Id think my parents were fucking morons.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.950486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>2094</td>\n",
       "      <td>YOU ARE NOTHING BUT A STUPID MORON</td>\n",
       "      <td>YOU ARE NOTHING BUT A STUPID MORON</td>\n",
       "      <td>True</td>\n",
       "      <td>0.950486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11135</th>\n",
       "      <td>2122</td>\n",
       "      <td>Goddamn you're dumb.</td>\n",
       "      <td>Goddamn you're dumb.</td>\n",
       "      <td>True</td>\n",
       "      <td>0.939145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24192</th>\n",
       "      <td>4620</td>\n",
       "      <td>What the fuck? You're literally defending nazi...</td>\n",
       "      <td>What the fuck? You're literally defending nazi...</td>\n",
       "      <td>True</td>\n",
       "      <td>0.933832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       conversation_idx                                               text  \\\n",
       "4852                928  the average human (including myself) is an ign...   \n",
       "7747               1468           Id think my parents were fucking morons.   \n",
       "10996              2094                 YOU ARE NOTHING BUT A STUPID MORON   \n",
       "11135              2122                               Goddamn you're dumb.   \n",
       "24192              4620  What the fuck? You're literally defending nazi...   \n",
       "\n",
       "                                                   imm_1  is_refusal  \\\n",
       "4852   the average human (including myself) is an ign...        True   \n",
       "7747            Id think my parents were fucking morons.        True   \n",
       "10996                 YOU ARE NOTHING BUT A STUPID MORON        True   \n",
       "11135                               Goddamn you're dumb.        True   \n",
       "24192  What the fuck? You're literally defending nazi...        True   \n",
       "\n",
       "       persp_toxicity  \n",
       "4852         0.950486  \n",
       "7747         0.950486  \n",
       "10996        0.950486  \n",
       "11135        0.939145  \n",
       "24192        0.933832  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = (\n",
    "    analysis_df.sort_values('persp_toxicity', ascending=False)\n",
    "    [['conversation_idx', 'text', 'imm_1', 'is_refusal', 'persp_toxicity']]\n",
    "    .head(5)\n",
    ")\n",
    "examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65975b5",
   "metadata": {},
   "source": [
    "### Topic highlights\n",
    "Load best runs from migrated LDA assets and surface top terms per split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a53301a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/l/Raja_win/cleaned/.venv/lib/python3.12/site-packages/sklearn/base.py:463: InconsistentVersionWarning: Trying to unpickle estimator LatentDirichletAllocation from version 1.5.1 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/l/Raja_win/cleaned/.venv/lib/python3.12/site-packages/sklearn/base.py:463: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.5.1 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/l/Raja_win/cleaned/.venv/lib/python3.12/site-packages/sklearn/base.py:463: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.5.1 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/l/Raja_win/cleaned/.venv/lib/python3.12/site-packages/sklearn/base.py:463: InconsistentVersionWarning: Trying to unpickle estimator CountVectorizer from version 1.5.1 when using version 1.8.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>top_terms</th>\n",
       "      <th>n_topics</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>asset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>woman white black rape man think like sex say ...</td>\n",
       "      <td>3</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>tfidf_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>mana nyx riki gem motherfucker fow eul treads ...</td>\n",
       "      <td>3</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>tfidf_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ethnicite revisionist democratically chancello...</td>\n",
       "      <td>3</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>tfidf_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>woman think like right say man want thing know...</td>\n",
       "      <td>2</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>tfidf_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>seti isch muscato danielle projects laxdelux b...</td>\n",
       "      <td>2</td>\n",
       "      <td>TfidfVectorizer</td>\n",
       "      <td>tfidf_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>life think good kill right want person thing b...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>trump ban country supporter vote support think...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>want life abortion think pay like say work cho...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>black white racist race racism like say think ...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>culture muslims kill like terrorist attack num...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5</td>\n",
       "      <td>woman sex like think man tran gender person wa...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6</td>\n",
       "      <td>country culture slave america right war israel...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7</td>\n",
       "      <td>rape woman man say think like consent sex want...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>8</td>\n",
       "      <td>gun weapon police law know suicide time shoot ...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>nazi violence group speech right like hate thi...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>right say gay think word like thing mean want ...</td>\n",
       "      <td>11</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_ref</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>white black race racist racism culture think l...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>child year kid change time old good know life ...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2</td>\n",
       "      <td>movie character film like art good think video...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>police crime violence black violent kill cop c...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4</td>\n",
       "      <td>car drug drive alcohol drunk drink like effect...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>military soldier army anthem gmo smart fight t...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>child parent right abortion sex want support f...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7</td>\n",
       "      <td>gender sex identity social like definition mea...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>health medical mental body doctor care treatme...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9</td>\n",
       "      <td>country war world american america nation isra...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>human life moral think argument person right b...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11</td>\n",
       "      <td>country year work terrorism number europe time...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>12</td>\n",
       "      <td>trump vote think party president like election...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>13</td>\n",
       "      <td>evidence believe investigation court kavanaugh...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>14</td>\n",
       "      <td>school student college teach teacher education...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>15</td>\n",
       "      <td>market company system sell use apple service t...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>16</td>\n",
       "      <td>say point claim argument think know evidence l...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>17</td>\n",
       "      <td>rape crime victim case death justice criminal ...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>18</td>\n",
       "      <td>pay money work job tax wage government like bu...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>19</td>\n",
       "      <td>law right government illegal legal state citiz...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20</td>\n",
       "      <td>woman man male gender female tran sex society ...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>21</td>\n",
       "      <td>gun weapon ban firearm mass right shooting dea...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>22</td>\n",
       "      <td>circumcision benefit self harm point live prot...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>23</td>\n",
       "      <td>animal eat meat dog kill food like human need ...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>24</td>\n",
       "      <td>right view speech conservative think liberal p...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>25</td>\n",
       "      <td>like think want thing say way know bad person ...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>26</td>\n",
       "      <td>game sport play team world player win good lea...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>27</td>\n",
       "      <td>religion god islam religious muslim christian ...</td>\n",
       "      <td>28</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>count_acc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    topic                                          top_terms  n_topics  \\\n",
       "0       0  woman white black rape man think like sex say ...         3   \n",
       "1       1  mana nyx riki gem motherfucker fow eul treads ...         3   \n",
       "2       2  ethnicite revisionist democratically chancello...         3   \n",
       "3       0  woman think like right say man want thing know...         2   \n",
       "4       1  seti isch muscato danielle projects laxdelux b...         2   \n",
       "5       0  life think good kill right want person thing b...        11   \n",
       "6       1  trump ban country supporter vote support think...        11   \n",
       "7       2  want life abortion think pay like say work cho...        11   \n",
       "8       3  black white racist race racism like say think ...        11   \n",
       "9       4  culture muslims kill like terrorist attack num...        11   \n",
       "10      5  woman sex like think man tran gender person wa...        11   \n",
       "11      6  country culture slave america right war israel...        11   \n",
       "12      7  rape woman man say think like consent sex want...        11   \n",
       "13      8  gun weapon police law know suicide time shoot ...        11   \n",
       "14      9  nazi violence group speech right like hate thi...        11   \n",
       "15     10  right say gay think word like thing mean want ...        11   \n",
       "16      0  white black race racist racism culture think l...        28   \n",
       "17      1  child year kid change time old good know life ...        28   \n",
       "18      2  movie character film like art good think video...        28   \n",
       "19      3  police crime violence black violent kill cop c...        28   \n",
       "20      4  car drug drive alcohol drunk drink like effect...        28   \n",
       "21      5  military soldier army anthem gmo smart fight t...        28   \n",
       "22      6  child parent right abortion sex want support f...        28   \n",
       "23      7  gender sex identity social like definition mea...        28   \n",
       "24      8  health medical mental body doctor care treatme...        28   \n",
       "25      9  country war world american america nation isra...        28   \n",
       "26     10  human life moral think argument person right b...        28   \n",
       "27     11  country year work terrorism number europe time...        28   \n",
       "28     12  trump vote think party president like election...        28   \n",
       "29     13  evidence believe investigation court kavanaugh...        28   \n",
       "30     14  school student college teach teacher education...        28   \n",
       "31     15  market company system sell use apple service t...        28   \n",
       "32     16  say point claim argument think know evidence l...        28   \n",
       "33     17  rape crime victim case death justice criminal ...        28   \n",
       "34     18  pay money work job tax wage government like bu...        28   \n",
       "35     19  law right government illegal legal state citiz...        28   \n",
       "36     20  woman man male gender female tran sex society ...        28   \n",
       "37     21  gun weapon ban firearm mass right shooting dea...        28   \n",
       "38     22  circumcision benefit self harm point live prot...        28   \n",
       "39     23  animal eat meat dog kill food like human need ...        28   \n",
       "40     24  right view speech conservative think liberal p...        28   \n",
       "41     25  like think want thing say way know bad person ...        28   \n",
       "42     26  game sport play team world player win good lea...        28   \n",
       "43     27  religion god islam religious muslim christian ...        28   \n",
       "\n",
       "         vectorizer      asset  \n",
       "0   TfidfVectorizer  tfidf_ref  \n",
       "1   TfidfVectorizer  tfidf_ref  \n",
       "2   TfidfVectorizer  tfidf_ref  \n",
       "3   TfidfVectorizer  tfidf_acc  \n",
       "4   TfidfVectorizer  tfidf_acc  \n",
       "5   CountVectorizer  count_ref  \n",
       "6   CountVectorizer  count_ref  \n",
       "7   CountVectorizer  count_ref  \n",
       "8   CountVectorizer  count_ref  \n",
       "9   CountVectorizer  count_ref  \n",
       "10  CountVectorizer  count_ref  \n",
       "11  CountVectorizer  count_ref  \n",
       "12  CountVectorizer  count_ref  \n",
       "13  CountVectorizer  count_ref  \n",
       "14  CountVectorizer  count_ref  \n",
       "15  CountVectorizer  count_ref  \n",
       "16  CountVectorizer  count_acc  \n",
       "17  CountVectorizer  count_acc  \n",
       "18  CountVectorizer  count_acc  \n",
       "19  CountVectorizer  count_acc  \n",
       "20  CountVectorizer  count_acc  \n",
       "21  CountVectorizer  count_acc  \n",
       "22  CountVectorizer  count_acc  \n",
       "23  CountVectorizer  count_acc  \n",
       "24  CountVectorizer  count_acc  \n",
       "25  CountVectorizer  count_acc  \n",
       "26  CountVectorizer  count_acc  \n",
       "27  CountVectorizer  count_acc  \n",
       "28  CountVectorizer  count_acc  \n",
       "29  CountVectorizer  count_acc  \n",
       "30  CountVectorizer  count_acc  \n",
       "31  CountVectorizer  count_acc  \n",
       "32  CountVectorizer  count_acc  \n",
       "33  CountVectorizer  count_acc  \n",
       "34  CountVectorizer  count_acc  \n",
       "35  CountVectorizer  count_acc  \n",
       "36  CountVectorizer  count_acc  \n",
       "37  CountVectorizer  count_acc  \n",
       "38  CountVectorizer  count_acc  \n",
       "39  CountVectorizer  count_acc  \n",
       "40  CountVectorizer  count_acc  \n",
       "41  CountVectorizer  count_acc  \n",
       "42  CountVectorizer  count_acc  \n",
       "43  CountVectorizer  count_acc  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_lda_results(path: Path) -> Optional[List[Dict]]:\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    with path.open('rb') as fp:\n",
    "        return pickle.load(fp)\n",
    "\n",
    "def choose_best(runs: Optional[List[Dict]]) -> Optional[Dict]:\n",
    "    if not runs:\n",
    "        return None\n",
    "    return max(\n",
    "        runs,\n",
    "        key=lambda r: (\n",
    "            r.get('coherence') if r.get('coherence') is not None else -1,\n",
    "            -(r.get('perplexity') or 0),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def topics_frame(entry: Dict, label: str) -> pd.DataFrame:\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            'topic': range(len(entry.get('topics', []))),\n",
    "            'top_terms': [' '.join(words) for words in entry.get('topics', [])],\n",
    "            'n_topics': entry.get('n_topics'),\n",
    "            'vectorizer': entry.get('vectorizer').__class__.__name__ if entry.get('vectorizer') else None,\n",
    "            'asset': label,\n",
    "        }\n",
    "    )\n",
    "\n",
    "lda_assets = {\n",
    "    'tfidf_ref': LDA_TFIDF_REF,\n",
    "    'tfidf_acc': LDA_TFIDF_ACC,\n",
    "    'count_ref': LDA_COUNT_REF,\n",
    "    'count_acc': LDA_COUNT_ACC,\n",
    "}\n",
    "\n",
    "topic_tables: List[pd.DataFrame] = []\n",
    "best_topics: Dict[str, Dict] = {}\n",
    "for label, path in lda_assets.items():\n",
    "    runs = load_lda_results(path)\n",
    "    best = choose_best(runs)\n",
    "    if best:\n",
    "        topic_tables.append(topics_frame(best, label))\n",
    "        best_topics[label] = {\n",
    "            'n_topics': best.get('n_topics'),\n",
    "            'top_terms': [' '.join(words) for words in best.get('topics', [])],\n",
    "        }\n",
    "\n",
    "pd.concat(topic_tables, ignore_index=True) if topic_tables else 'No topic assets loaded.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040dae84",
   "metadata": {},
   "source": [
    "### Export report snapshot\n",
    "Persist key metrics and topic highlights to assets/processed/report/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06b989f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/l/Raja_win/cleaned/report/report_more_refuse.json')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPORT_PATH = REPORT_DIR / f'report_{SOURCE_MODE}.json'\n",
    "report_payload = {\n",
    "    'source_mode': SOURCE_MODE,\n",
    "    'source_path': str(SOURCE_PATH),\n",
    "    'refusal_true_rate_threshold': REFUSAL_TRUE_RATE_THRESHOLD if SOURCE_MODE == 'more_refuse' else None,\n",
    "    'high_toxicity_threshold': HIGH_TOXICITY,\n",
    "    'refusal_conversation_threshold': REFUSAL_CONVERSATION_THRESHOLD,\n",
    "    'metrics': summary_metrics,\n",
    "    'topic_highlights': best_topics,\n",
    "}\n",
    "\n",
    "with REPORT_PATH.open('w') as fp:\n",
    "    json.dump(report_payload, fp, indent=2)\n",
    "\n",
    "REPORT_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cleaned",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
